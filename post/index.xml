<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Enterprising &amp; Concentrating</title>
    <link>/post/</link>
    <description>Recent content in Posts on Enterprising &amp; Concentrating</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018 Dong</copyright>
    <lastBuildDate>Sun, 01 Jan 2017 00:00:00 +0800</lastBuildDate>
    <atom:link href="/post/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>2018-01-23-English-note</title>
      <link>/post/2018-01-23-english-note/</link>
      <pubDate>Tue, 23 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-01-23-english-note/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;be central to&lt;/li&gt;
&lt;li&gt;be cast as&lt;/li&gt;
&lt;li&gt;convex凸面&lt;/li&gt;
&lt;li&gt;psyched激动的兴奋的&lt;/li&gt;
&lt;li&gt;main grant主要资助&lt;/li&gt;
&lt;li&gt;under-used较少使用&lt;/li&gt;
&lt;li&gt;vehemently opposed强烈反对&lt;/li&gt;
&lt;li&gt;Gauging测量&lt;/li&gt;
&lt;li&gt;Raisin葡萄干&lt;/li&gt;
&lt;li&gt;neural network神经网络&lt;/li&gt;
&lt;li&gt;This study is a sequel to the reports of是某研究的后续&lt;/li&gt;
&lt;li&gt;competing model如果同时有多个模型候选，那么彼此是竞争模型&lt;/li&gt;
&lt;li&gt;is appropriate for；is also capable of适于&lt;/li&gt;
&lt;li&gt;expensive and tme-consuming to collect 收集起来耗时费力&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;highly limited&lt;/strong&gt; in availability 非常有限&lt;/li&gt;
&lt;li&gt;are essental for 重要的&lt;/li&gt;
&lt;li&gt;epidemiology&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Producing models&lt;/strong&gt; that can predict abundance &lt;strong&gt;well&lt;/strong&gt;, with good resolution over large areas, has &lt;strong&gt;therefore&lt;/strong&gt; been &lt;strong&gt;an important aim&lt;/strong&gt; in ecology, but &lt;strong&gt;poses&lt;/strong&gt; considerable challenges. 带来极大的挑战&lt;/li&gt;
&lt;li&gt;have driven an explosion in the use of these techniques&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;none of these has produced consistently satisfactory results and each has signifcant theoretcal or practcal limitatons&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;with very low precision in predictons&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;none have yet performed well enough to be of use for&lt;/strong&gt; many real-world applicatons&lt;/li&gt;
&lt;li&gt;envisage设想&lt;/li&gt;
&lt;li&gt;are a signifcant improvement on previously widely available distributon data&lt;/li&gt;
&lt;li&gt;custom-writen定制编写&lt;/li&gt;
&lt;li&gt;burgeoning adj. 增长迅速的；生机勃勃的&lt;/li&gt;
&lt;li&gt;a standard method&lt;/li&gt;
&lt;li&gt;A &lt;strong&gt;substantial&lt;/strong&gt; number of&lt;/li&gt;
&lt;li&gt;in aggregate with联合&lt;/li&gt;
&lt;li&gt;A controversy &lt;strong&gt;ensued&lt;/strong&gt; as to whether much of this risk could be explained随之而来的&lt;/li&gt;
&lt;li&gt;be likely to&lt;/li&gt;
&lt;li&gt;persecution迫害&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Approximately&lt;/strong&gt; half of the book is written, and all completed chapters are now available as online preview.&lt;/li&gt;
&lt;li&gt;the same issues &lt;strong&gt;arise&lt;/strong&gt; over and over&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;I have attempted to&lt;/strong&gt; collect &lt;strong&gt;my accumulated knowledge&lt;/strong&gt; from these &lt;strong&gt;interactions&lt;/strong&gt; &lt;strong&gt;in the form of&lt;/strong&gt; this book.&lt;/li&gt;
&lt;li&gt;As of this writing截至撰写本文&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;please also feel free to&lt;/strong&gt; record these suggestions as issues on github&lt;/li&gt;
&lt;li&gt;with no &lt;strong&gt;manual&lt;/strong&gt; post-processing手动&lt;/li&gt;
&lt;li&gt;brush up擦亮，提高&lt;/li&gt;
&lt;li&gt;We are also grateful to&lt;/li&gt;
&lt;li&gt;prime minister&lt;/li&gt;
&lt;li&gt;humiliating defeat丧权辱国&lt;/li&gt;
&lt;li&gt;take the plunge冒险尝试；采取决定性步骤&lt;/li&gt;
&lt;li&gt;is contingent upon视…而定&lt;/li&gt;
&lt;li&gt;predisposing&lt;/li&gt;
&lt;li&gt;substantive information实质性信息&lt;/li&gt;
&lt;li&gt;the specific accomplishments特定的成就&lt;/li&gt;
&lt;li&gt;productive collaboration高效的协作&lt;/li&gt;
&lt;li&gt;In many cases&lt;/li&gt;
&lt;li&gt;discrete&lt;/li&gt;
&lt;li&gt;detectable&lt;/li&gt;
&lt;li&gt;significantly associated SNPs&lt;/li&gt;
&lt;li&gt;raise the risk by&lt;/li&gt;
&lt;li&gt;be effective in elucidating&lt;/li&gt;
&lt;li&gt;placed in&lt;/li&gt;
&lt;li&gt;easily accessible databases&lt;/li&gt;
&lt;li&gt;increasingly affordable costs&lt;/li&gt;
&lt;li&gt;in the context of&lt;/li&gt;
&lt;li&gt;analytic challenges&lt;/li&gt;
&lt;li&gt;most critical&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;thorny&lt;/strong&gt; problem&lt;/li&gt;
&lt;li&gt;Although this was problematic&lt;/li&gt;
&lt;li&gt;viewed it as a potential strength&lt;/li&gt;
&lt;li&gt;sparked&lt;/li&gt;
&lt;li&gt;the creation of appropriate software&lt;/li&gt;
&lt;li&gt;it was anticipated&lt;/li&gt;
&lt;li&gt;follow-up studies随访研究&lt;/li&gt;
&lt;li&gt;Saccone &lt;strong&gt;and colleagues&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;evidence supporting the &lt;strong&gt;biological relevance&lt;/strong&gt; of each associated SNP&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>2018-01-20-English-note</title>
      <link>/post/2018-01-20-english-note/</link>
      <pubDate>Sat, 20 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-01-20-english-note/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;seeding progeny&lt;/strong&gt; were &lt;strong&gt;raised in&lt;/strong&gt; containers&lt;/li&gt;
&lt;li&gt;stem cutting&lt;/li&gt;
&lt;li&gt;across a range of sites and conditions&lt;/li&gt;
&lt;li&gt;single-tree non-contiguous plots&lt;/li&gt;
&lt;li&gt;pilodyn penetration (PIL)用来测定木材密度&lt;/li&gt;
&lt;li&gt;are defined as above&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\otimes\)&lt;/span&gt; denotes the Kronecker product operation&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\oplus\)&lt;/span&gt; denotes the direct sum operation&lt;/li&gt;
&lt;li&gt;analysis of DBH&lt;/li&gt;
&lt;li&gt;assess the statistical significance评估统计显著性&lt;/li&gt;
&lt;li&gt;in previous single-site analyses预分析&lt;/li&gt;
&lt;li&gt;make use of利用&lt;/li&gt;
&lt;li&gt;take advantage of利用&lt;/li&gt;
&lt;li&gt;manipulate this problem处理问题&lt;/li&gt;
&lt;li&gt;to deal with this issue&lt;/li&gt;
&lt;li&gt;there is obvious evidence有明显的证据&lt;/li&gt;
&lt;li&gt;A considerable body of research on大量&lt;/li&gt;
&lt;li&gt;Theoretical work理论工作&lt;/li&gt;
&lt;li&gt;quantitative variation&lt;/li&gt;
&lt;li&gt;plays a &lt;strong&gt;pronounced&lt;/strong&gt; role in显著的&lt;/li&gt;
&lt;li&gt;empirical investigations实证研究，对应的是理论研究&lt;/li&gt;
&lt;li&gt;One of the reasons for this may be due to the fact that&lt;/li&gt;
&lt;li&gt;methodological limitation方法局限&lt;/li&gt;
&lt;li&gt;owing to由于&lt;/li&gt;
&lt;li&gt;a portion of一份&lt;/li&gt;
&lt;li&gt;a fraction of一部分&lt;/li&gt;
&lt;li&gt;be relaxed somewhat缓解&lt;/li&gt;
&lt;li&gt;in the case&lt;/li&gt;
&lt;li&gt;the question has not been explored这个问题尚未探索/探讨 staggered摇摇晃晃地 veiled &lt;strong&gt;chameleons&lt;/strong&gt;变色龙 masterpiece杰作 stunning极好的 sexually harass性骚扰 synopsis大纲 syllabus大纲 rejoice高兴 apostrophe撇&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>2018-01-20-ericjang</title>
      <link>/post/2018-01-20-ericjang/</link>
      <pubDate>Sat, 20 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-01-20-ericjang/</guid>
      <description>&lt;p&gt;Eric Jang&lt;/p&gt;
&lt;p&gt;Technology, A.I., Careers&lt;/p&gt;
&lt;p&gt;Wednesday, January 17, 2018 &lt;a href=&#34;http://blog.evjang.com/2018/01/nf1.html&#34;&gt;&lt;/a&gt; Normalizing Flows Tutorial, Part 1: Distributions and Determinants# E If you are a machine learning practitioner working on generative modeling, Bayesian deep learning, or deep reinforcement learning, normalizing flows are a handy technique to have in your algorithmic toolkit. Normalizing flows transform simple densities (like Gaussians) into rich complex distributions that can be used for generative models, RL, and variational inference. TensorFlow has a nice set of functions that make it easy to build flows and train them to suit real-world data.&lt;/p&gt;
&lt;p&gt;This tutorial comes in two parts: Part 1: Distributions and Determinants. In this post, I explain how invertible transformations of densities can be used to implement more complex densities, and how these transformations can be chained together to form a “normalizing flow”. Part 2: Modern Normalizing Flows: In a follow-up post, I survey recent techniques developed by researchers to learn normalizing flows, and explain how a slew of modern generative modeling techniques – autoregressive models, MAF, IAF, NICE, Real-NVP, Parallel-Wavenet – are all related to each other. This series is written for an audience with a rudimentary understanding of linear algebra, probability, neural networks, and TensorFlow. Knowledge of recent advances in Deep Learning, generative models will be helpful in understanding the motivations and context underlying these techniques, but they are not necessary.&lt;/p&gt;
&lt;p&gt;Background&lt;/p&gt;
&lt;p&gt;Statistical Machine Learning algorithms try to learn the structure of data by fitting a parametric distribution &lt;span class=&#34;math inline&#34;&gt;\(p(x; \theta)\)&lt;/span&gt; to it. Given a dataset, if we can represent it with a distribution, we can: Generate new data “for free” by sampling from the learned distribution in silico; no need to run the true generative process for the data. This is a useful tool if the data is expensive to generate, i.e. a real-world experiment that takes a long time to run [1]. Sampling is also used to construct estimators of high-dimensional integrals over spaces. Evaluate the likelihood of data observed at test time (this can be used for rejection sampling or to score how good our model is). Find the conditional relationship between variables. For example, learning the distribution &lt;span class=&#34;math inline&#34;&gt;\(p(x_2 | x_1)\)&lt;/span&gt; allows us to build discriminative classification or regression models. Score our algorithm by using complexity measures like entropy, mutual information, and moments of the distribution. We’ve gotten pretty good at sampling (1), as evidenced by recent work on generative models for images and audio. These kinds of generative models are already being deployed in real commercial applications and Google products.&lt;/p&gt;
&lt;p&gt;However, the research community currently directs less attention towards unconditional &amp;amp; conditional likelihood estimation (2, 3) and model scoring (4). For instance, we don’t know how to compute the support of a GAN decoder (how much of the output space has been assigned nonzero probability by the model), we don’t know how to compute the density of an image with respect to a DRAW distribution or even a VAE, and we don’t know how to analytically compute various metrics (KL, earth-mover distance) on arbitrary distributions, even if we know their analytic densities.&lt;/p&gt;
&lt;p&gt;Generating likely samples isn’t enough: we also care about answering “how likely is the data?” [2], having flexible conditional densities (e.g. for sampling/evaluating divergences of multi-modal policies in RL), and being able to choose rich families of priors and posteriors in variational inference.&lt;/p&gt;
&lt;p&gt;Consider for a moment, your friendly neighborhood Normal Distribution. It’s the Chicken Soup of distributions: we can draw samples from it easily, we know its analytic density and KL divergence to other Normal distributions, the central limit theorem gives us confidence that we can apply it to pretty much any data, and we can even backprop through its samples via the reparameterization trick. The Normal Distribution’s ease-of-use makes it a very popular choice for many generative modeling and reinforcement learning algorithms.&lt;/p&gt;
&lt;p&gt;Unfortunately, the Normal distribution just doesn’t cut it in many real-world problems we care about. In Reinforcement Learning – especially continuous control tasks such as robotics – policies are often modeled as multivariate Gaussians with diagonal covariance matrices.&lt;/p&gt;
&lt;p&gt;By construction, uni-modal Gaussians cannot do well on tasks that require sampling from a multi-modal distribution. A classic example of where uni-modal policies fail is an agent trying to get to its house across a lake. It can get home by circumventing the lake clockwise (left) or counterclockwise (right), but a Gaussian policy is not able to represent two modes. Instead, it chooses actions from a Gaussian whose mean is a linear combination of the two modes, resulting in the agent going straight into the icy water. Sad!&lt;/p&gt;
&lt;p&gt;The above example illustrates how the Normal distribution can be overly simplistic. In addition to bad symmetry assumptions, Gaussians have most of their density concentrated at the edges in high dimensions and are not robust to rare events. Can we find a better distribution with the following properties?&lt;/p&gt;
&lt;p&gt;Complex enough to model rich, multi-modal data distributions like images and value functions in RL environments? … while retaining the easy comforts of a Normal distribution: sampling, density evaluation, and with re-parameterizable samples? The answer is yes! Here are a few ways to do it: Use a mixture model to represent a multi-modal policy, where a categorical represents the “option” and the mixture represents the sub-policy. This provides samples that are easy to sample and evaluate, but samples are not trivially re-parameterizable, which makes them hard to use for VAEs and posterior inference. However, using a Gumbel-Softmax / Concrete relaxation of the categorical “option” would provide a multi-modal, re-parameterizable distribution. Autoregressive factorizations of policy / value distributions. In particular, discrete distributions (e.g. Categorical) have the ability to model arbitrary discrete distributions. In RL, one can avoid this altogether by symmetry-breaking the value distribution via recurrent policies, noise, or distributional RL. This helps by collapsing the complex value distributions into simpler conditional distributions at each timestep. Learning with energy-based models, a.k.a undirected graphical models with potential functions that eschew an normalized probabilistic interpretation. Here’s a recent example of this applied to RL. Normalizing Flows: learn invertible, volume-tracking transformations of distributions that we can manipulate easily.&lt;/p&gt;
&lt;p&gt;Let’s explore the last approach - Normalizing Flows.&lt;/p&gt;
&lt;p&gt;Change of Variables, Change of Volume&lt;/p&gt;
&lt;p&gt;Let’s build up some intuition by examining linear transformations of 1D random variables. Let &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; be the distribution &lt;span class=&#34;math inline&#34;&gt;\(\text{Uniform}(0,1)\)&lt;/span&gt;. Let random variable &lt;span class=&#34;math inline&#34;&gt;\(Y = f(X) = 2X + 1\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is a simple affine (scale &amp;amp; shift) transformation of the underlying “source distribution” &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. What this means is that a sample &lt;span class=&#34;math inline&#34;&gt;\(x^i\)&lt;/span&gt; from &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; can be converted into a sample from &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; by simply applying the function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; to it.&lt;/p&gt;
&lt;p&gt;The green square represents the shaded probability mass on &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}\)&lt;/span&gt; for both &lt;span class=&#34;math inline&#34;&gt;\(p(x)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(p(y)\)&lt;/span&gt; - the height represents the density function at that value. Observe that because probability mass must integrate to 1 for any distribution, the act of scaling the domain by 2 everywhere means we must divide the probability density by 2 everywhere, so that the total area of the green square and blue rectangle are the same (=1).&lt;/p&gt;
&lt;p&gt;If we zoom in on a particular x and an infinitesimally nearby point &lt;span class=&#34;math inline&#34;&gt;\(x+dx\)&lt;/span&gt;, then applying f to them takes us to the pair &lt;span class=&#34;math inline&#34;&gt;\((y, y+dy)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;On the left, we have a locally increasing function (&lt;span class=&#34;math inline&#34;&gt;\(dy/dx &amp;gt; 0\)&lt;/span&gt;) and on the right, a locally decreasing function (&lt;span class=&#34;math inline&#34;&gt;\(dy/dx &amp;lt; 0\)&lt;/span&gt;). In order to preserve total probability, the change of &lt;span class=&#34;math inline&#34;&gt;\(p(x)\)&lt;/span&gt; along interval &lt;span class=&#34;math inline&#34;&gt;\(dx\)&lt;/span&gt; must be equivalent to the change of &lt;span class=&#34;math inline&#34;&gt;\(p(y)\)&lt;/span&gt; along interval &lt;span class=&#34;math inline&#34;&gt;\(dy\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p(x) dx = p(y) dy\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In order to conserve probability, we only care about the amount of change in y and not its direction (it doesn’t matter if &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; is increasing or decreasing at x, we assume the amount of change in y is the same regardless). Therefore, &lt;span class=&#34;math inline&#34;&gt;\(p(y) = p(x) | dx/dy |\)&lt;/span&gt;. Note that in log-space, this is equivalent to &lt;span class=&#34;math inline&#34;&gt;\(\log p(y) = \log p(x) + \log | dx/dy |\)&lt;/span&gt;. Computing log-densities is more well-scaled for numerical stability reasons.&lt;/p&gt;
&lt;p&gt;Now let’s consider the multivariate case, with 2 variables. Again, zooming into an infinitesimally small region of our domain, our initial “segment” of the base distribution is now a square with width dx.&lt;/p&gt;
&lt;p&gt;Note that a transformation that merely shifts a rectangular patch &lt;span class=&#34;math inline&#34;&gt;\((x1,x2, x3,x4)\)&lt;/span&gt; does not change the area. We are only interested in the rate of change per unit area of x, so the displacement &lt;span class=&#34;math inline&#34;&gt;\(dx\)&lt;/span&gt; can be thought of as a unit of measure, which is arbitrary. To make the following analysis simple and unit-less, let’s investigate a unit square on the origin, i.e. 4 points &lt;span class=&#34;math inline&#34;&gt;\((0,0), (1,0), (0,1), (1,1)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Multiplying this by the matrix &lt;span class=&#34;math inline&#34;&gt;\([[a, b];[c, d]]\)&lt;/span&gt; will take points on this square into a parallelogram, as shown on the figure to the right (below). &lt;span class=&#34;math inline&#34;&gt;\((0,0)\)&lt;/span&gt; is sent to &lt;span class=&#34;math inline&#34;&gt;\((0,0)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\((1,0)\)&lt;/span&gt; is sent to &lt;span class=&#34;math inline&#34;&gt;\((a,b)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\((1,0)\)&lt;/span&gt; sent to &lt;span class=&#34;math inline&#34;&gt;\((c,d)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\((1,1)\)&lt;/span&gt; sent to &lt;span class=&#34;math inline&#34;&gt;\((a+c,b+d)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Thus, a unit square in the domain of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; corresponds to a deformed parallelogram in the domain of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, so the per-unit rate of change in area is the area of the parallelogram, i.e. &lt;span class=&#34;math inline&#34;&gt;\(ad - bc\)&lt;/span&gt;. The area of a parallelogram, &lt;span class=&#34;math inline&#34;&gt;\(ad - bc\)&lt;/span&gt;, is nothing more than the absolute value of the determinant of the linear transformation!&lt;/p&gt;
&lt;p&gt;In 3 dimensions, the “change in area of parallelogram” becomes a “change in volume of parallelpiped”, and even higher dimensions, this becomes “change in volume of a n-parallelotope”. But the concept remains the same - determinants are nothing more than the amount (and direction) of volume distortion of a linear transformation, generalized to any number of dimensions.&lt;/p&gt;
&lt;p&gt;What if the transformation f is nonlinear? Instead of a single parallelogram that tracks the distortion of any point in space, you can picture many infinitesimally small parallelograms corresponding to the amount of volume distortion for each point in the domain. Mathematically, this locally-linear change in volume is &lt;span class=&#34;math inline&#34;&gt;\(|\text{det}(J(f^{-1}(x)))|\)&lt;/span&gt;, where J(f^-1(x)) is the Jacobian of the function inverse - a higher-dimensional generalization of the quantity dx/dy from before.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y = f(x)\]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[p(y) = p(f^{-1}(y)) \cdot |\text{det} J(f^{-1}(y))| = \log p(f^{-1}(y)) + \log |\text{det}(J(f^{-1}(y)))|\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;When I learned about determinants in middle &amp;amp; high school I was very confused at the seemingly arbitrary definition of determinants. We were only taught how to compute a determinant, instead of what a determinant meant: the local, linearized rate of volume change of a transformation.&lt;/p&gt;
&lt;p&gt;Transformed Distributions in TensorFlow&lt;/p&gt;
&lt;p&gt;TensorFlow has an elegant API for transforming distributions. A TransformedDistribution is specified by a base distribution object that we will transform, and a Bijector object that implements 1) a forward transformation &lt;span class=&#34;math inline&#34;&gt;\(y = f(x)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(f : \mathbb{R}^d → \mathbb{R}^d\)&lt;/span&gt; 2) its inverse transformation &lt;span class=&#34;math inline&#34;&gt;\(x = f^-1(y)\)&lt;/span&gt;, and 3) the inverse log determinant of the Jacobian &lt;span class=&#34;math inline&#34;&gt;\(\log |\text{det}J (f^-1(y))|\)&lt;/span&gt;. For the rest of this post, I will abbreviate this quantity as ILDJ.&lt;/p&gt;
&lt;p&gt;Under this abstraction, forward sampling is trivial:&lt;/p&gt;
&lt;p&gt;bijector.forward(base_dist.sample())&lt;/p&gt;
&lt;p&gt;To evaluate log-density of the transformed distribution:&lt;/p&gt;
&lt;p&gt;distribution.log_prob(bijector.inverse(x)) + bijector.inverse_log_det_jacobian(x)&lt;/p&gt;
&lt;p&gt;Furthermore, if bijector.forward is a differentiable function, then Y = bijector.forward(x) is a re-parameterizable distribution with respect to samples x = base_distribution.sample(). This means that normalizing flows can be used as a drop-in replacement for variational posteriors in a VAE (as an alternative to a Gaussian).&lt;/p&gt;
&lt;p&gt;Some commonly used TensorFlow distributions are actually implemented using these TransformedDistributions.&lt;/p&gt;
&lt;p&gt;Source Distribution Bijector.forward Transformed Distribution&lt;/p&gt;
&lt;p&gt;Normal exp(x) LogNormal Exp(rate=1) -log(x) Gumbel(0,1) Gumbel(0,1) Softmax(x) Gumbel-Softmax / Concrete&lt;/p&gt;
&lt;p&gt;Under standard convention, TransformedDistributions are named as &lt;span class=&#34;math inline&#34;&gt;\(\text{Bijector}^{-1}\text{BaseDistribution}\)&lt;/span&gt; so an ExpBijector applied to a Normal distribution becomes LogNormal. There are some exceptions to this naming scheme - the Gumbel-Softmax distribution is implemented as the RelaxedOneHotCategorical distribution, which applies a SoftmaxCentered bijector to a Gumbel distribution.&lt;/p&gt;
&lt;p&gt;Normalizing Flows and Learning Flexible Bijectors&lt;/p&gt;
&lt;p&gt;Why stop at 1 bijector? We can chain any number of bijectors together, much like we chain layers together in a neural network [3]. This is construct is known as a “normalizing flow”. Additionally, if a bijector has tunable parameters with respect to bijector.log_prob, then the bijector can actually be learned to transform our base distribution to suit arbitrary densities. Each bijector functions as a learnable “layer”, and you can use an optimizer to learn the parameters of the transformation to suit our data distribution we are trying to model. One algorithm to do this is maximum likelihood estimation, which modifies our model parameters so that our training data points have maximum log-probability under our transformed distribution. We compute and optimize over log probabilities rather than probabilities for numerical stability reasons.&lt;/p&gt;
&lt;p&gt;This slide from Shakir Mohamed and Danilo Rezende’s UAW talk (slides) that illustrates this concept:&lt;/p&gt;
&lt;p&gt;However, computing the determinant of an arbitrary &lt;span class=&#34;math inline&#34;&gt;\(N \times N\)&lt;/span&gt; Jacobian matrix has runtime complexity &lt;span class=&#34;math inline&#34;&gt;\(O(N^3)\)&lt;/span&gt;, which is very expensive to put in a neural network. There is also the trouble of inverting an arbitrary function approximator. Much of the current research on Normalizing Flows focuses on how to design expressive Bijectors that exploit GPU parallelism during forward and inverse computations, all while maintaining computationally efficient ILDJs.&lt;/p&gt;
&lt;p&gt;Code Example&lt;/p&gt;
&lt;p&gt;Let’s build a basic normalizing flow in TensorFlow in about 100 lines of code. This code example will make use of:&lt;/p&gt;
&lt;p&gt;TF Distributions - general API for manipulating distributions in TF. For this tutorial you’ll need TensorFlow r1.5 or later. TF Bijector - general API for creating operators on distributions Numpy, Matplotlib.&lt;/p&gt;
&lt;p&gt;We are trying to model the distribution &lt;span class=&#34;math inline&#34;&gt;\(p(x_1, x_2) = \mathcal{N}(x1|\mu=1/4x_2^2, \sigma=1) \cdot N(x_2|\mu=0, \sigma=4)\)&lt;/span&gt;. We can generate samples from the target distribution using the following code snippet (we generate them in TensorFlow to avoid having to copy samples from the CPU to the GPU on each minibatch):&lt;/p&gt;
&lt;p&gt;For our base distribution, we’ll use an Isotropic Gaussian.&lt;/p&gt;
&lt;p&gt;Next, we construct the bijector and create a TransformedDistribution from it. Let’s build a flow that resembles a standard fully-connected network, i.e. alternating matrix multiplication with nonlinearities.&lt;/p&gt;
&lt;p&gt;The Jacobian of an affine function is trivial to compute, but worst case determinants are &lt;span class=&#34;math inline&#34;&gt;\(O(n^3)\)&lt;/span&gt;, which is unacceptably slow to compute. Instead, TensorFlow provides a structured affine transformation whose determinant can be computed more efficiently. This Affine transform is parameterized as a lower triangular matrix &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; plus a low rank update:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[M + V \cdot D \cdot V^T\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To compute &lt;span class=&#34;math inline&#34;&gt;\(\text{det}(M + V \cdot D \cdot V^T)\)&lt;/span&gt; cheaply, we use the matrix determinant lemma.&lt;/p&gt;
&lt;p&gt;Next, we need an invertible nonlinearity in order to express non-linear functions (otherwise the chain of affine bijectors remains affine). Sigmoid / tanh may seem like good choices, but they are incredibly unstable to invert - small changes in the output near -1 or 1 correspond to massive changes in input. In my experiments I could not chain 2 saturating nonlinearities together without gradients exploding. Meanwhile, ReLU is stable, but not invertible for &lt;span class=&#34;math inline&#34;&gt;\(x &amp;lt; 0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;I chose to implement PReLU (parameterized ReLU), which is the same as Leaky ReLU but with a learnable slope in the negative regime. The simplicity of PReLU and its straightforward Jacobian makes for a nice exercise in implementing your own custom Bijectors: notice that the ILDJ is 0 when &lt;span class=&#34;math inline&#34;&gt;\(x &amp;gt; 0\)&lt;/span&gt; (no volume change) and &lt;span class=&#34;math inline&#34;&gt;\(1/\alpha\)&lt;/span&gt; otherwise (compensating for the contraction in volume from multiplying x by &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;PReLU is an element-wise transformation, so the Jacobian is diagonal. The determinant of a diagonal matrix is just the product of the diagonal entries, so we compute the ILDJ by simply summing the diagonal entries of the log-Jacobian [4]. We build the “MLP Bijector” by using tfb.Chain(), then apply it to our base distribution to create the transformed distribution:&lt;/p&gt;
&lt;p&gt;Finally, we’ll train the model using Maximum Likelihood estimation: maximize the expected log probability of samples from the real data distribution, under our choice of model.&lt;/p&gt;
&lt;p&gt;We can visualize the (slow) deformation of space by coloring samples from base distribution according to their starting quadrant,&lt;/p&gt;
&lt;p&gt;And that’s it! TensorFlow distributions makes normalizing flows to implement, and automatically accumulate all the Jacobians determinants in a way that is clean and highly readable. Full source code for this post can be found on Github.&lt;/p&gt;
&lt;p&gt;You might notice that the deformation is rather slow, and it takes a lot of layers to learn a rather simple transformation [5]. In the next post, I will cover more modern techniques for learning normalizing flows.&lt;/p&gt;
&lt;p&gt;Acknowledgements&lt;/p&gt;
&lt;p&gt;I am very grateful to Dustin Tran for clarifying my understanding of normalizing flows, Luke Metz, Katherine Lee, and Samy Bengio for proofreading this post, and to Ben Poole, Rif A. Saurous, Ian Langmore for helping me to debug my code. You rock!&lt;/p&gt;
&lt;p&gt;Footnotes&lt;/p&gt;
&lt;p&gt;[1] The notion that we can augment our dataset with &lt;em&gt;new&lt;/em&gt; information from a finite set of data is a rather disturbing one, and it remains to be shown whether probabilistic machine learning can truly replace true generative processes (e.g. simulation of fluid dynamics), or whether at the end of the day it is only good for amortizing computation and any generalization we get on the training / test distribution is a lucky accident. [2] See A note on the evaluation of generative models for a thought-provoking discussion about how high log-likelihood is neither sufficient nor necessary to generate “plausible” images. Still, it’s better than nothing and in practice a useful diagnostic tool. [3]There’s a connection between Normalizing Flows and GANs via encoder-decoder GAN architectures that learn the inverse of the generator (ALI / BiGAN). Since there is a separate encoder trying to recover &lt;span class=&#34;math inline&#34;&gt;\(u = G^{-1}(X)\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(X = G(u)\)&lt;/span&gt;, the generator can be thought of as a flow for the simple uniform distribution. However, we don’t know how to compute the amount of volume expansion/contraction w.r.t. X, so we cannot recover density from GANs. However, it’s probably not entirely unreasonable to model the log-det-jacobian numerically or enforce some kind of linear-time Jacobian by construction. [4] The lemma “Determinant of diagonal matrices is the product of the diagonal entries” is quite intuitive from a geometric point of view: each dimension’s length distortion is independent of the other dimensions, so the total volume change is just the product of changes in each direction, as if we were computing the volume of a high-dimensional rectangular prism. [5] This MLP is rather limited in capacity because each affine transformation is only a 2x2 matrix, and the PReLU “warps” the underlying distribution very slowly (so several PreLUs are needed to bend the data into the right shape). For low dimensional distributions, this MLP is a very poor choice of a normalizing flow, and is meant for educational purposes.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>2018-01-20-purrr</title>
      <link>/post/2018-01-20-purrr/</link>
      <pubDate>Sat, 20 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-01-20-purrr/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Wu1996-克隆重复试验检测上位方差</title>
      <link>/post/2018-01-20-wu1996/</link>
      <pubDate>Sat, 20 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-01-20-wu1996/</guid>
      <description>&lt;div class=&#34;section level1&#34;&gt;
&lt;h1&gt;摘要&lt;/h1&gt;
&lt;p&gt;定量遗传模型是利用已知的克隆复制家族结构&lt;strong&gt;将遗传变异分解为其加性，显性和上位性成分&lt;/strong&gt;的一种定量遗传模型。利用后代测试，这个模型是基于这样一个理论：来自实验设计的线性模型的方差分量可以用亲属之间的期望协方差来表示。然而，&lt;strong&gt;如果一对数量性状位点（QTL）之间的相互作用解释了总体上位性的大部分，则将严重高估加性和显性差异，但低估了上位性差异。&lt;/strong&gt;在本文中，开发了一个新的模型来处理这个问题，将父母和子女的材料结合到同一个测试中。在上述条件下，新模型可以为加性x加性方差提供准确的估计。此外，其估计优势和总的上位变异的准确性远远大于以前的模型的准确性。但是，&lt;strong&gt;如果有明显证据显示高阶相互作用，尤其是&amp;gt;4个QTL对总上位性的主要贡献，以前的模型更适合将数量性状的遗传方差划分&lt;/strong&gt;。对杨树阶乘交配设计的一个例子的再分析显示，当使用两个不同的假设（低与高阶的上位相互作用）时，估计新模型和先前模型之间的方差分量存在很大差异。这个新模型将成为估计物种数量遗传模式的一种替代方法，特别是对长寿的，主要是异交的林木，可以进行克隆复制。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level1&#34;&gt;
&lt;h1&gt;介绍&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;上位性&lt;/strong&gt;，或者不同但功能相关基因座上的等位基因之间的遗传相互作用，是基因和特征之间非线性关系的结果（Falconer 1989）。生物化学，生理学和生物遗传学研究的大量研究强烈地表明，数量性状中无处不在的位点间互作（Hayman和Mather，1955; Hayman，1958; Wright，1980; Mather and Jinks，1982）。 Sewall Wright对进化论的最大贡献之一是显示&lt;strong&gt;上位性在表型进化和物种形成中如何起着核心作用&lt;/strong&gt;（Wright 1932,1980; Templeton 1979,1980; Carson and Tempteton 1984; Provine 1986; Wade 1992）。理论工作表明，&lt;strong&gt;野生种群的数量变异可能带有潜在的强上位性成分&lt;/strong&gt;（Crow和Kimura，1970），因为&lt;strong&gt;这些成分的小表型效应往往隐藏着更大的分子效应&lt;/strong&gt;。因此，&lt;strong&gt;当基因型频率受选择或种群瓶颈严重干扰时，神秘的上位变异可能变得重要&lt;/strong&gt;（Carson和Templeton 1984; Bryant等1986; Goodnight 1987,1988; Tachida和Cockerham 1989; Bryant和Meffert 1992; Wade 1992年）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;上位可能也是杂种优势的重要原因。&lt;/strong&gt; Jinks和Jones（1958）通过将不同的上位性成分整合到遗传模型中，观察到杂种优势的存在与否以及一年生植物中是否存在非等位相互作用。在Minvielle（1987）的双轨迹双列模型中，&lt;strong&gt;杂种优势可能来自乘性上位相互作用而不是显性效应&lt;/strong&gt;。 （Wright 1922; Dempster 1942; Ricky 1942; Williams 1959; Griffing 1990; Schnell and Cockerham 1992）研究了繁殖积累对复杂性状杂种优势的贡献。&lt;/p&gt;
&lt;p&gt;尽管上位性在进化和育种中起着显着的作用，但是对这种现象的实证研究惊人地少。造成这种现象的原因之一可能是由于&lt;strong&gt;分离世代以检测上位性不能在许多物种中获得&lt;/strong&gt;（Mather and Jinks 1982）。即使在目前的基于分子标记的图谱中，也存在处理这个问题的方法限制（Tanksley 1993）。 Fisher（1981）首先介绍了遗传方差分解为其加性，显性和上位性组分。 Cockerham（1954）将上位分量进一步细分为附加x加性，加性x显性和显性x显性的相互作用。所有这些&lt;strong&gt;相互作用可以根据世代均值来估计&lt;/strong&gt;（Hayman和Mather，1955）。福斯特（Foster）和肖（Shaw，1988）利用克隆从总遗传方差中分离上位性，通过表达亲缘关系期望协方差的实验设计线性模型的方差分量（Cockerham，1963）。由于其强大的性质，福斯特 - 肖的方法已被应用于许多森林树种（例如，Foster和Shaw 1988; Foster 1990; Mullin等1992; Mullin和Part 1994; R6nnberg-Wiistljung等1994）。 不幸的是，&lt;strong&gt;Foster-Shaw的方法不能提供遗传方差分量的准确估计&lt;/strong&gt;。一方面，&lt;strong&gt;半同胞或者同胞之间由协方差估计的加性和显性方差受到一部分上位性，尤其是低阶相互作用的污染&lt;/strong&gt;（Cockerham，1963）。另一方面，&lt;strong&gt;对上位变异的估计仅仅是家族变异所包含的上位性的一部分，而不是总的上位变异&lt;/strong&gt;。尽管假定多个位点之间有强的高度相互作用（至少_&amp;gt; 3）（Mullin和Park，1992），有偏倚的估计值可能会有所缓解，但在存在双位点相互作用的情况下，这将是误导。&lt;strong&gt;当低阶相互作用形成上位性的一部分时，估计的加性和显性方差将被严重污染，并且估计的上位性方差仅为总上位性的四分之一&lt;/strong&gt;。到目前为止，尚未探索基于不同交互类型如何准确排除上位性的问题。 在本文中，我试图通过在交配设计中结合父母和子代信息，在假设二基因上位 的情况下检测更精确的上位性估计。 从精确性分析和杨树实例可以看出，新方法可以作为将数量性状的遗传方差划分为可克隆物种的因果成分的替代方法。&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>2018-01-14-English-note</title>
      <link>/post/2018-01-14-english-note/</link>
      <pubDate>Sun, 14 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-01-14-english-note/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;data retrieval数据检索&lt;/li&gt;
&lt;li&gt;deemed to be视为&lt;/li&gt;
&lt;li&gt;irretrievable loss不可挽回的损失&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;patiently waiting&lt;/strong&gt; for &lt;strong&gt;optimum maturity&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Leftover lunch剩饭&lt;/li&gt;
&lt;li&gt;interactive visualizations&lt;/li&gt;
&lt;li&gt;Hello good &lt;strong&gt;people of Great Britain&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;When Rex Tillerson visited New Zealand last year, he &lt;strong&gt;stated&lt;/strong&gt; he had never seen so many middle fingers &lt;strong&gt;aimed at&lt;/strong&gt; his motorcade.&lt;/li&gt;
&lt;li&gt;Sometimes looks are &lt;strong&gt;deceiving&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;instant translate&lt;/li&gt;
&lt;li&gt;Impediments&lt;/li&gt;
&lt;li&gt;Nearly half of the &lt;strong&gt;residents&lt;/strong&gt; of low-income countries live more than one hour’s travel from a city, where vital resources such as education and health care are concentrated&lt;/li&gt;
&lt;li&gt;airport runway机场跑道&lt;/li&gt;
&lt;li&gt;Racial resentment种族仇恨&lt;/li&gt;
&lt;li&gt;Justice and dignity尊严, the endless shortage&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Excited to be joining&lt;/strong&gt; the … Biostatistics Department in February!&lt;/li&gt;
&lt;li&gt;hands-on&lt;/li&gt;
&lt;li&gt;ON MY DAY OFF休息日&lt;/li&gt;
&lt;li&gt;wearable sensor&lt;/li&gt;
&lt;li&gt;grant-in-aid资助款&lt;/li&gt;
&lt;li&gt;the &lt;strong&gt;aegis&lt;/strong&gt; of保护&lt;/li&gt;
&lt;li&gt;capitalize on利用&lt;/li&gt;
&lt;li&gt;tantamount to相当于&lt;/li&gt;
&lt;li&gt;in lieu of替代&lt;/li&gt;
&lt;li&gt;cheat sheet速查表&lt;/li&gt;
&lt;li&gt;Why might you do otherwise? 你为什么要这样做呢？&lt;/li&gt;
&lt;li&gt;sheepishly adv. 羞怯地；愚蠢地&lt;/li&gt;
&lt;li&gt;religiously adv. 虔诚地，笃信地&lt;/li&gt;
&lt;li&gt;highly internally consistent内部高度一致&lt;/li&gt;
&lt;li&gt;US authorities美国当局&lt;/li&gt;
&lt;li&gt;poring凝视&lt;/li&gt;
&lt;li&gt;transaction交易&lt;/li&gt;
&lt;li&gt;cash withdrawal现金提取&lt;/li&gt;
&lt;li&gt;inauguration就职&lt;/li&gt;
&lt;li&gt;There is nothing we can’t overcome.没有什么是我们解决不了的&lt;/li&gt;
&lt;li&gt;Donald J. Trump&lt;/li&gt;
&lt;li&gt;Let me be very clear here&lt;/li&gt;
&lt;li&gt;Today, we &lt;strong&gt;witnessed&lt;/strong&gt; an &lt;strong&gt;incredible&lt;/strong&gt; moment in history&lt;/li&gt;
&lt;li&gt;furiously猛烈地&lt;/li&gt;
&lt;li&gt;internship实习生&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>invoke|调用，引用</title>
      <link>/post/2018-01-14-invoke/</link>
      <pubDate>Sun, 14 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-01-14-invoke/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;&lt;p&gt;You create a command line object specifying the options (e.g. the input filename and the output filename), then &lt;strong&gt;invoke&lt;/strong&gt; this command line via a Python operating system call (e.g. using the subprocess module). (Chapman et al., 2000)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;By piping the result to wc we &lt;strong&gt;invoke&lt;/strong&gt; the word count program which indicates how many lines (i.e., rows) have the &amp;gt; symbol (this should be 101 for this example). (Pevsner, 2009)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Because of the high copy number of retrotransposons in the genomes of extant pine species, it is tempting to &lt;strong&gt;invoke&lt;/strong&gt; the mechanism of reverse transcription in the generation of complex pine gene families. (Kinlaw, Neale, 1997)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Proofs given in the statistical literature of the latter part either assume that = 0, which is a relatively easy case, or &lt;strong&gt;invoke&lt;/strong&gt; the same result by Laha (1956) that was mentioned earlier. (Khuri, 2009)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>retrieve|检索，恢复</title>
      <link>/post/2018-01-14-retrieve/</link>
      <pubDate>Sun, 14 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-01-14-retrieve/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;&lt;p&gt;This task is undertaken at the NCBI in the form of Entrez Genome, the section of the NCBI sequence &lt;strong&gt;retrieval&lt;/strong&gt; system concerned with genomes and individual genome assembly versions, and the sequences of individual whole genome shotgun reads are also available. (Valentin et al., 2007)序列检索系统&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Archiving spectra required a tremendous amount of magnetic storage and the problems of &lt;strong&gt;retrieval&lt;/strong&gt; were becoming insurmountable. (Lodder, 2002)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;This abundance of material, assembled to guard against its &lt;strong&gt;irretrievable&lt;/strong&gt; loss, has intensified the problems of how best to conserve it and how to use it in plant breeding. (Brown, 1989)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A sophisticated two-level EM algorithm was implemented to estimate and &lt;strong&gt;retrieve&lt;/strong&gt; the missing information of segregation characterized by dominant-segregating markers such as single methylation polymorphisms. (Zhu et al., 2016)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>2018-01-12-English note</title>
      <link>/post/2018-01-12-english_note/</link>
      <pubDate>Fri, 12 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-01-12-english_note/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;start from scratch从头开始&lt;/li&gt;
&lt;li&gt;in a sense在某种意义上&lt;/li&gt;
&lt;li&gt;matrix factorization矩阵因子分解&lt;/li&gt;
&lt;li&gt;interrogate
&lt;ul&gt;
&lt;li&gt;Their application here can corroborate established evidence for the causal role of smoking in lung cancer as well as &lt;strong&gt;interrogate&lt;/strong&gt; the causal role for methylation as a mediating mechanism. (Teschendorff, Relton, 2017)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;be embedded in嵌入&lt;/li&gt;
&lt;li&gt;tiresome令人讨厌的&lt;/li&gt;
&lt;li&gt;elementary row operation初等行运算&lt;/li&gt;
&lt;li&gt;Echelon Form阶梯形式&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>R-error-collects</title>
      <link>/post/2018-01-12-r-error-collects/</link>
      <pubDate>Fri, 12 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-01-12-r-error-collects/</guid>
      <description>&lt;div class=&#34;section level1&#34;&gt;
&lt;h1&gt;检查一个字符串里是否有空格的代码&lt;/h1&gt;
&lt;pre&gt;&lt;code&gt;grepl(&amp;quot;\\s&amp;quot;, your_string)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;stopifnot&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;stopifnot只能停止不能自定义错误信息&lt;/h1&gt;
&lt;p&gt;应该用&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;if ()
  stop(
    msg(cli::rule(center = crayon::bold(&amp;quot;RM the spetial char in your title!&amp;quot;)),
          startup = TRUE)
    )&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>resort to 依靠 求助于</title>
      <link>/post/2018-01-12-resort/</link>
      <pubDate>Fri, 12 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-01-12-resort/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;&lt;p&gt;In order to discover novel motifs, we must &lt;strong&gt;resort to&lt;/strong&gt; de novo motif-finding algorithms. (Valentin et al., 2007)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Many longitudinal data models &lt;strong&gt;resort to&lt;/strong&gt; structured covariances which, although positive-definite and computationally favorable due to a 1068 reduced number of parameters, are possibly highly biased. (Yap, Fan, Wu, 2009)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;When approximate methods are intractable or result in insufficient accuracy, we must &lt;strong&gt;resort to&lt;/strong&gt; numerical integration. (Carlin, Louis, Carlin, 2009)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;As a result, g should be chosen as the prior as a last &lt;strong&gt;resort&lt;/strong&gt;, when methods of finding a better approximation have failed. (Carlin, Louis, Carlin, 2009)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>矩阵代数学习笔记</title>
      <link>/post/2018-01-12-%E7%9F%A9%E9%98%B5%E4%BB%A3%E6%95%B0%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</link>
      <pubDate>Fri, 12 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-01-12-%E7%9F%A9%E9%98%B5%E4%BB%A3%E6%95%B0%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;我越来越喜欢矩阵代数了，深深被它的魅力所吸引。总结一些笔记，包括概念和方法。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div id=&#34;-&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;什么是 齐次线性方程组？&lt;/h1&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;线性方程组&lt;/h2&gt;
&lt;p&gt;先说一下什么是 &lt;a href=&#34;https://zh.wikipedia.org/zh-hans/%E7%BA%BF%E6%80%A7%E6%96%B9%E7%A8%8B%E7%BB%84&#34;&gt;线性方程组&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\({\displaystyle {\begin{cases}a_{1,1}x_{1}+a_{1,2}x_{2}+\cdots +a_{1,n}x_{n}=b_{1}\\a_{2,1}x_{1}+a_{2,2}x_{2}+\cdots +a_{2,n}x_{n}=b_{2}\\\vdots \quad \quad \quad \vdots \\a_{m,1}x_{1}+a_{m,2}x_{2}+\cdots +a_{m,n}x_{n}=b_{m}\end{cases}}}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中的&lt;span class=&#34;math inline&#34;&gt;\(a_{1,1}, \, a_{1,2}\)&lt;/span&gt;以及&lt;span class=&#34;math inline&#34;&gt;\(b_{1}, \, b_{2}\)&lt;/span&gt;等等是已知的常数，而&lt;span class=&#34;math inline&#34;&gt;\(x_{1}, \, x_{2}\)&lt;/span&gt;等等则是要求的未知数。&lt;/p&gt;
&lt;p&gt;如果用线性代数中的概念来表达，则线性方程组可以写成：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{A} \mathbf{x} = \mathbf{b}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;这里的&lt;i&gt;A&lt;/i&gt;是&lt;i&gt;m&lt;/i&gt;×&lt;i&gt;n&lt;/i&gt;矩阵，&lt;b&gt;x&lt;/b&gt;是含有&lt;i&gt;n&lt;/i&gt;个元素列向量&lt;/a&gt;，&lt;b&gt;b&lt;/b&gt;是含有&lt;i&gt;m&lt;/i&gt; 个元素列向量。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\({\displaystyle A={\begin{bmatrix}a_{1,1}&amp;amp;a_{1,2}&amp;amp;\cdots &amp;amp;a_{1,n}\\a_{2,1}&amp;amp;a_{2,2}&amp;amp;\cdots &amp;amp;a_{2,n}\\\vdots &amp;amp;\vdots &amp;amp;\ddots &amp;amp;\vdots \\a_{m,1}&amp;amp;a_{m,2}&amp;amp;\cdots &amp;amp;a_{m,n}\end{bmatrix}},\quad {\mathbf {x}}={\begin{bmatrix}x_{1}\\x_{2}\\\vdots \\x_{n}\end{bmatrix}},\quad {\mathbf {b}}={\begin{bmatrix}b_{1}\\b_{2}\\\vdots \\b_{m}\end{bmatrix}}}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;这是线性方程组的另一种记录方法。&lt;strong&gt;在已知矩阵 &lt;span class=&#34;math inline&#34;&gt;\({\displaystyle A}\)&lt;/span&gt;和向量&lt;span class=&#34;math inline&#34;&gt;\({\displaystyle \mathbf {b}}\)&lt;/span&gt;的情况求得未知向量&lt;span class=&#34;math inline&#34;&gt;\({\displaystyle \mathbf {x} }\)&lt;/span&gt;是线性代数的基本问题之一。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;我们可以将线性方程组的求解问题看成&lt;strong&gt;向量&lt;span class=&#34;math inline&#34;&gt;\({\displaystyle \mathbf {b}}\)&lt;/span&gt;在矩阵&lt;span class=&#34;math inline&#34;&gt;\({\displaystyle A}\)&lt;/span&gt;所张成的线性空间里面的投影的问题&lt;/strong&gt;。未知数的个数如果是一般的n个的话，可以想象每个方程代表了n维空间里面的一个超平面。而方程组的解就是所有超平面的公共点。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;齐次线性方程组齐次线性方程组&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;齐次的线性方程组&lt;/strong&gt;是指向量&lt;span class=&#34;math inline&#34;&gt;\({\displaystyle \mathbf {b} =0}\)&lt;/span&gt;的情况。这时候方程变成：&lt;span class=&#34;math inline&#34;&gt;\({\displaystyle A\mathbf {x} =0}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;这个方程肯定会有一组解：&lt;span class=&#34;math inline&#34;&gt;\({\displaystyle \mathbf {x}=0}\)&lt;/span&gt;。实际上，方程的解就是矩阵 &lt;span class=&#34;math inline&#34;&gt;\({\displaystyle A}\)&lt;/span&gt;对应的线性变换的零空间。一般来说，当方程的个数小于未知数的个数时，方程组会有除 &lt;span class=&#34;math inline&#34;&gt;\({\displaystyle \mathbf {x} =0}\)&lt;/span&gt;以外的解。当方程组个数变多时，则要看其中“有效”的方程的个数。有时候某一个方程可以表示成另外几个方程的线性组合。比如方程组：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[{\displaystyle {\begin{cases}3x_{1}+x_{2}+2x_{3}=0\\x_{1}-x_{2}+4x_{3}=0\\2x_{1}+3x_{3}=0\end{cases}}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;之中，第三个方程就可以表示为前两个方程的线性组合：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[{\displaystyle 2x_{1}+3x_{3}={\frac {1}{2}}(3x_{1}+x_{2}+2x_{3})+{\frac {1}{2}}(x_{1}-x_{2}+4x_{3})}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;这时第三个方程组就可以不必考虑了。用线性代数的词汇表达，“有效”的方程的个数就是矩阵&lt;span class=&#34;math inline&#34;&gt;\({\displaystyle A}\)&lt;/span&gt;中线性无关的行向量的个数，或者说行向量线性张成的空间的维数。这个数也被称为矩阵的&lt;strong&gt;秩&lt;/strong&gt;。当矩阵&lt;span class=&#34;math inline&#34;&gt;\({\displaystyle A}\)&lt;/span&gt;的秩&lt;span class=&#34;math inline&#34;&gt;\({\displaystyle r}\)&lt;/span&gt;小于未知数的个数&lt;span class=&#34;math inline&#34;&gt;\({\displaystyle n}\)&lt;/span&gt;时，方程组的解会有无穷多个，构成一个&lt;span class=&#34;math inline&#34;&gt;\({\displaystyle n-r}\)&lt;/span&gt;维的线性空间。而当&lt;span class=&#34;math inline&#34;&gt;\({\displaystyle r}\)&lt;/span&gt;等于未知数的个数&lt;span class=&#34;math inline&#34;&gt;\({\displaystyle n}\)&lt;/span&gt;时，方程组有唯一零解，&lt;span class=&#34;math inline&#34;&gt;\({\displaystyle r}\)&lt;/span&gt;不可能大于&lt;span class=&#34;math inline&#34;&gt;\({\displaystyle n}\)&lt;/span&gt;。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;section level1&#34;&gt;
&lt;h1&gt;矩阵分解&lt;/h1&gt;
&lt;div id=&#34;lu&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;LU分解&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;LU分解&lt;/strong&gt;得名于分解的两部分：下（lower）三角和上（upper）三角矩阵&lt;span class=&#34;citation&#34;&gt;(Howard 2017)&lt;/span&gt;（P76）使得方阵&lt;span class=&#34;math inline&#34;&gt;\({\displaystyle A}\)&lt;/span&gt;有&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[{\displaystyle A=LU}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[A \mathbf{x} = \mathbf{b}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;所以有&lt;span class=&#34;math display&#34;&gt;\[L(U \mathbf{x})=\mathbf{b}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;因为&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{x}\)&lt;/span&gt;是一个向量，所以&lt;span class=&#34;math inline&#34;&gt;\(U \mathbf{x}\)&lt;/span&gt;还是一个向量，令&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{t} = U \mathbf{x}\)&lt;/span&gt;，于是有 &lt;span class=&#34;math display&#34;&gt;\[L \mathbf{t} = \mathbf{b}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;这样如果能先解出t就能得到x。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cholesky&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Cholesky分解（将一个对称正定矩阵分解为一个矩阵和其转置的积）&lt;/h2&gt;
&lt;p&gt;满足&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[{\displaystyle A=LL^*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(L^*\)&lt;/span&gt;是&lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;的共轭转置矩阵&lt;a href=&#34;#fn1&#34; class=&#34;footnoteRef&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;因为我们的处理对象都是实数，所以&lt;span class=&#34;math inline&#34;&gt;\(L^*\)&lt;/span&gt;就是&lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;的转置矩阵。&lt;/p&gt;
&lt;p&gt;Cholesky分解只适用于对称正定矩阵&lt;a href=&#34;#fn2&#34; class=&#34;footnoteRef&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;。&lt;/p&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-Howard2017&#34;&gt;
&lt;p&gt;Howard, James Patrick. 2017. &lt;em&gt;Computational methods for numerical analysis with R&lt;/em&gt;. 1st ed. Chapman &amp;amp; Hall/Crc Numerical Analysis and Scientific Computing Series. Chapman; Hall/;CRC Press. &lt;a href=&#34;http://gen.lib.rus.ec/book/index.php?md5=6957AA74C02676D2FF39712D80535E9E&#34; class=&#34;uri&#34;&gt;http://gen.lib.rus.ec/book/index.php?md5=6957AA74C02676D2FF39712D80535E9E&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;将复数理解为复平面，则复共轭无非是对实轴的反射。&lt;a href=&#34;#fnref1&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;一个n×n的实对称矩阵 &lt;span class=&#34;math inline&#34;&gt;\({\displaystyle M}\)&lt;/span&gt; 是正定的，当且仅当对于所有的非零实系数向量z，都有&lt;span class=&#34;math inline&#34;&gt;\(z^T {\displaystyle M} z &amp;gt; 0\)&lt;/span&gt;。&lt;a href=&#34;#fnref2&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>统计建模中的一切都可以看作是一个回归</title>
      <link>/post/regression/</link>
      <pubDate>Fri, 12 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/regression/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;来自siminaboca &lt;a href=&#34;https://github.com/SiminaB/Mentoring/blob/master/Regression.Rmd&#34;&gt;Github的一篇文章&lt;/a&gt;，由Google翻译。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这当然有点夸张！ 这也取决于强调的方面，例如统计研究生课程。 我知道我自己的培训项目高度集中于把大量的统计数据看作是一种特殊情况，或者是某种线性回归模型的推广。 我很喜欢这个观点，只要我能和合作者合作，就尽量强调这一点，但我承认，有时你必须从特殊情况出发才能建立起整体。&lt;/p&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;回归概念与损失函数和条件均值和中值相关&lt;/h2&gt;
&lt;p&gt;“回归”一词目前以各种各样的方式使用。一般情况下，如果有一个称为结果变量或因变量的变量&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;，以及一些其他称为解释变量或自变量的变量&lt;span class=&#34;math inline&#34;&gt;\(X_1\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(X_2\)&lt;/span&gt;等，则一个&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;在&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;s上的回归可以是 &lt;a href=&#34;http://www.jstor.org/stable/2727353&#34;&gt;给定&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;（或“以&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;为条件”）&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;的“概率分布的任何特征”&lt;/a&gt; 。这些特征通常涉及分布的“平均”或“中心”的一些度量，例如均值或中值。&lt;/p&gt;
&lt;p&gt;最简单的回归方法就是具有单一解释变量的线性回归。换句话说，试图通过散点图来绘制“最佳拟合线”。有很多方法来定义“最佳拟合”，而通常考虑的方法是基于&lt;a href=&#34;https://en.wikipedia.org/wiki/Loss_function#Quadratic_loss_function&#34;&gt;二次损失函数quadratic loss function&lt;/a&gt; （也称为平方误差损失或&lt;span class=&#34;math inline&#34;&gt;\(L_2\)&lt;/span&gt;损失）。基本上，如果试图&lt;strong&gt;最小化平方和的误差&lt;/strong&gt;，即真实结果Y和&lt;span class=&#34;math inline&#34;&gt;\(\hat{Y}\)&lt;/span&gt;上的拟合值之间的差异，解决方案是“常规usual” 的&lt;a href=&#34;https://en.wikipedia.org/wiki/Ordinary_least_squares&#34;&gt;普通最小二乘法ordinary least squares (OLS)&lt;/a&gt;。拿这个例子来说，有一个单一的解释变量&lt;span class=&#34;math inline&#34;&gt;\(X_1\)&lt;/span&gt;，结果变量&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;只是增加了一些噪音。回归OLS拟合标蓝色，拟合值与Y的真实值之间的线段也是蓝色。其他两行以橙色虚线显示。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../post/Regression_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;误差平方为OLS拟合的总和6.19，而其他两条线的误差平方是7.58和10.19。&lt;/p&gt;
&lt;p&gt;如果没有解释变量，则拟合线是平坦的，代表结果值的平均值（平均值）：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../post/Regression_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;对于上面的例子，这显然是不合适的。事实上，在这种情况下，平方误差的总和是49.34！&lt;/p&gt;
&lt;p&gt;一般来说，OLS有一些很好的属性，例如，如果噪声项定义为&lt;span class=&#34;math inline&#34;&gt;\(\epsilon = Y - \beta_0 - \beta_1 X_1\)&lt;/span&gt;，其中&lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt;是真正的截距和斜率线——是有限方差的独立同分布，OLS拟合是条件均值E(Y|X)的无偏估计（即其均值等于条件均值），实际上是 &lt;a href=&#34;https://en.wikipedia.org/wiki/Minimum-variance_unbiased_estimator&#34;&gt;最小方差无偏估计量minimum-variance unbiased estimator&lt;/a&gt;。请注意，到目前为止，我还没有讨论Y的分布，甚至Y是否连续。然而，在噪声项（因此Y是以X为条件）正态分布的情况下，OLS也是条件均值E(Y|X)的最大似然估计（maximum likelihood estimator，MLE）。&lt;/p&gt;
&lt;p&gt;为什么常常考虑二次损失函数的一个原因是因为数学运算非常好（可以使用通常的基于微积分的二阶导数测试来求解最大值）。如果存在更多的解释变量，这也可以很好地推广 - 我们可以将它们全部包含在矩阵X中，并且在一些多元微积分的帮助下基本上以相同的方式进行。然而，还有许多其他的方式来定义“最适合”。然而，也许有人有兴趣估计之外的Y|X分布的一个方面而非其平均值，比如它的中位数。中位数自然地连接到 &lt;a href=&#34;https://en.wikipedia.org/wiki/Loss_function&#34;&gt;绝对损失absolute loss&lt;/a&gt;（也称为&lt;span class=&#34;math inline&#34;&gt;\(L_1\)&lt;/span&gt;损失），同样，均值就联系到二次误差损失。在这种情况下，标准是&lt;strong&gt;最小化误差的绝对值的总和&lt;/strong&gt;，也称为 &lt;a href=&#34;https://en.wikipedia.org/wiki/Least_absolute_deviations&#34;&gt;最小绝对偏差（least absolute deviations，LAD）或最小绝对残差（least absolute residuals）&lt;/a&gt;。在没有解释变量的情况下，平面拟合线表示解释值的中值。如果我们更广义地看一个函数f，其中的值f(X)最为相似的Y，有相似的使用二次损失来定义，那么“最适合” &lt;a href=&#34;https://web.stanford.edu/~hastie/Papers/ESLII.pdf&#34;&gt;最大限度地减少预期的预测误差expected prediction error（EPE -见统计学习基础 &lt;em&gt;Elements of statistical learning&lt;/em&gt; 第2.4节）&lt;/a&gt;是函数f(x) = E(Y|X=x)（条件均值）; 相反，如果使用绝对损失来定义，则为f(x)=中值median(Y|X=x)（条件中值）。一般来说，中值比均值方法更稳健，因为它们缺乏对异常值的依赖性，但由于不连续的求导复杂化了优化程序，数学变得更加困难。对于上面的例子，无论是最小化平方和还是误差的绝对值总和，结果都几乎相同：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;##intercept and slope estimated via OLS利用OLS估计截距和斜率
coef(lm(y ~ x1))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept)          x1 
##   0.1263162   0.9645116&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(quantreg)
##intercept and slope estimated via quantile regression with median (see below)利用分位数回归和中值估计截距和斜率
coef(rq(y ~ x1, tau=0.5))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept)          x1 
##   0.1218339   0.9931817&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;类似地，如果使用 &lt;a href=&#34;http://www.jstor.org/stable/2727353&#34;&gt;阶跃函数（0-1）损失step function (0-1) loss&lt;/a&gt; ，则结果为f(x) = mode(Y|X=x) (conditional mode)。请注意，均值，中位数和模式是描述连续分布的“中心”的最常用方式。&lt;/p&gt;
&lt;p&gt;考虑除中位数之外的另一个分位数也可能是有意义的，当考虑具有不对称分布的变量（例如收入分配）时有时是这种情况。 &lt;a href=&#34;https://en.wikipedia.org/wiki/Quantile_regression&#34;&gt;分位数回归Quantile regression&lt;/a&gt;为此考虑了不同的损失函数; 用于中位数的损失函数等同于LAD。如果噪声项具有不对称的拉普拉斯分布，则在此获得的用于特定分位数的估计量就是其 &lt;a href=&#34;http://www.american.edu/cas/economics/info-metrics/pdf/upload/working-paper-bera.pdf&#34;&gt;MLE&lt;/a&gt;。对于本文档的其余部分，考虑到平方误差损失，我们通常会以“常规方式”使用“回归”。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ols&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;更多关于OLS拟合：线性代数和几何解释&lt;/h2&gt;
&lt;p&gt;上面我们考虑了二维拟合线的特殊情况，其中只有一个解释变量，并估计了两个参数：“最佳拟合线”的斜率和截距。一般来说，可以考虑许多变量。通常将所有这些变量考虑在一个单一的矩阵&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;中，每个变量都作为列，每个样本作为一行，第一列通常由1组成，目标是估计一个“参数”&lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;向量，所以结果&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;是“尽可能接近”到&lt;span class=&#34;math inline&#34;&gt;\(X\beta\)&lt;/span&gt;（因此，&lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;的第一个元素是截距）。在一定的线性代数假设下， &lt;a href=&#34;https://en.wikipedia.org/wiki/Proofs_involving_ordinary_least_squares&#34;&gt;多元情况下的OLS有一个很好的封闭形式&lt;/a&gt;：&lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta} = (X&amp;#39;X)^{-1}X&amp;#39;Y\)&lt;/span&gt;。这意味着拟合值是：&lt;span class=&#34;math inline&#34;&gt;\(\hat{Y}=X\hat{\beta} = X(X&amp;#39;X)^{-1}X&amp;#39;Y\)&lt;/span&gt;。矩阵&lt;span class=&#34;math inline&#34;&gt;\(X(X&amp;#39;X)^{-1}X&amp;#39;\)&lt;/span&gt;实际上是 &lt;a href=&#34;https://en.wikipedia.org/wiki/Ordinary_least_squares&#34;&gt;投影矩阵在由X的列所跨越的空间上&lt;/a&gt;，这是另一种概念化&lt;span class=&#34;math inline&#34;&gt;\(\hat{Y}\)&lt;/span&gt;表示最接近向量是解释变量的线性组合的所有向量中的&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;。我们再一次使用&lt;span class=&#34;math inline&#34;&gt;\(L^2\)&lt;/span&gt;损失定义“最接近的”，或者在线性代数术语中，最小化&lt;span class=&#34;math inline&#34;&gt;\(L^2\)&lt;/span&gt;的范数 &lt;span class=&#34;math inline&#34;&gt;\(||.||\)&lt;/span&gt;：&lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta} = \arg\min||Y-X\beta||\)&lt;/span&gt;。值得注意的是，解&lt;span class=&#34;math inline&#34;&gt;\(X\beta = Y\)&lt;/span&gt;中的&lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;实际上代表一个 &lt;a href=&#34;https://en.wikipedia.org/wiki/Ordinary_least_squares&#34;&gt;超定系统overdetermined system&lt;/a&gt; 因此通常无法得到准确解——因此，我们寻找一种解决方案，给出一个“接近”Y的拟合，同时处于解释变量所跨越的空间中。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;t&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;T检验、方差分析，及其他：线性模型中的假设检验&lt;/h2&gt;
&lt;p&gt;也许大多数人首先想到的统计方法是t检验。 &lt;a href=&#34;https://en.wikipedia.org/wiki/Student%27s_t-test#Independent_two-sample_t-test&#34;&gt;学生氏两样本t-test&lt;/a&gt; 用于&lt;strong&gt;比较两个分布的均值&lt;/strong&gt;，对于正态分布的随机变量或对于具有足够大的样本大小的连续随机变量，&lt;strong&gt;零假设表明它们是相等的&lt;/strong&gt;。关于线性回归的一个强有力的事情是假定等式的两样本t检验实际上与线性模型相同，其中解释变量是定义组成员关系的0/1变量：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(380148)
n &amp;lt;- 10
##define the two random variables定义2个随机变量
##y1 is from a normal distribution with mean=0, sd=1
y1 &amp;lt;- rnorm(n,0,1)
##y2 is from a normal distribution with mean=0.5, sd=1
y2 &amp;lt;- rnorm(n,0.5,1)
##run a t-test!
t.test(y1,y2,var.equal = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Two Sample t-test
## 
## data:  y1 and y2
## t = -0.66034, df = 18, p-value = 0.5174
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -1.5638050  0.8158546
## sample estimates:
## mean of x mean of y 
## 0.2961889 0.6701641&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;##create a single vector of outcomes and a vector indicating group membership表示组关系的向量
y &amp;lt;- c(y1,y2)
x1 &amp;lt;- c(rep(0,length(y1)),rep(1,length(y2)))
summary(lm(y~x1))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = y ~ x1)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.3130 -0.9469  0.1265  1.0192  1.7432 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)
## (Intercept)   0.2962     0.4005    0.74    0.469
## x1            0.3740     0.5663    0.66    0.517
## 
## Residual standard error: 1.266 on 18 degrees of freedom
## Multiple R-squared:  0.02365,    Adjusted R-squared:  -0.03059 
## F-statistic: 0.436 on 1 and 18 DF,  p-value: 0.5174&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;我们可以看到，p值是相同的，估计的斜率实际上等于平均值之间的差异。由于OLS属性背后的一个假设是噪声项是相同分布的，这就意味着方差不等的情况不能直接放在这个框架中。但是，如果方差不相等，这意味着也许应该考虑其他解释变量，所以之后这些变量可以包含在回归模型中。&lt;/p&gt;
&lt;p&gt;ANOVA也是同样的思想：要比较多组均值。拥有回归思维方式的好处之一是，用条件方法解释结果是非常容易的，它也允许一个统一的框架，它可以包括连续和离散的解释变量以及它们之间的相互作用，而不必求助于像双向ANOVA，ANCOVA等特殊术语&lt;/p&gt;
&lt;p&gt;一般而言，我们可以通过执行F检验来测试多变量模型中参数的任何线性组合是否等于0 。这是一个强有力的方法，因为它包括两个系数相等的情况，当然，单个参数是否等于0有一个特殊情况。对于后一种情况，这种方法相当于使用$ t np-1 &lt;span class=&#34;math inline&#34;&gt;\(自由度，其中\)&lt;/span&gt; p $是不包括截距的参数的数量。因此，回归方法再一次考虑了一个更一般的框架，特殊情况自然就会出现。&lt;/p&gt;
&lt;p&gt;The same idea holds in the case of ANOVA, where multiple group means are being compared. One of the benefits of having a regression mindset is that it very easy to interpret the results in terms of conditional means and it also allows for a unified framework which can include both continuous and discrete explanatory variables and interactions between them, without having to resort to special terminology like two-way ANOVA, ANCOVA, etc.&lt;/p&gt;
&lt;p&gt;In general, we can test whether any linear combination of parameters in a multivariate model is equal to 0 by performing an &lt;a href=&#34;https://courses.washington.edu/b515/l6.pdf&#34;&gt;F-test&lt;/a&gt;. This is a powerful approach, as it includes cases including the equality of two coefficients and of course, has a special case whether a single parameter is equal to 0. For this latter case, this approach is equivalent to using a t-test with &lt;span class=&#34;math inline&#34;&gt;\(n-p-1\)&lt;/span&gt; degrees of freedom, where &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is the number of parameters, excluding the intercept. Thus, once again, the regression approach allows for a more general framework from which the special cases naturally fall out.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;from-weighted-regression-to-generalized-linear-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;From weighted regression to generalized linear models&lt;/h2&gt;
&lt;p&gt;We mentioned before that one of the properties that is generally assumed for linear regression is that the noise terms are independent and identically distributed. In case this does not hold, an &lt;a href=&#34;https://en.wikipedia.org/wiki/Generalized_linear_model&#34;&gt;alterative to OLS&lt;/a&gt; is to use either &lt;em&gt;weighted least squares&lt;/em&gt; (WLS) or &lt;em&gt;generalized least squares&lt;/em&gt; (GLS) approaches. This means that if the variance-covariance matrix of the noise terms is &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;, then the MLE solution is: &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}=(X&amp;#39;W^{-1}X)^{-1}X&amp;#39;W^{-1}Y\)&lt;/span&gt;. This is equivalent to minimizing &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta} = \arg\min(Y-X\beta)&amp;#39;W^{-1}(Y-X\beta)\)&lt;/span&gt;, known as the &lt;em&gt;Mahalanobis length,&lt;/em&gt; instead of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta} = \arg\min||Y-X\beta||\)&lt;/span&gt; as before. If W has all off-diagonal terms equal to 0 (i.e. the noise terms are independent), this is the WLS estimate, otherwise it is the GLS estimate.&lt;/p&gt;
&lt;p&gt;What happens if the outcome is not normally distributed? Here we come to the &lt;a href=&#34;https://en.wikipedia.org/wiki/Generalized_linear_model&#34;&gt;generalized linear model&lt;/a&gt;. In the case in which the outcome Y is from a class of distributions known as the &lt;a href=&#34;https://en.wikipedia.org/wiki/Exponential_family&#34;&gt;exponential family&lt;/a&gt; - which include the normal, exponential, and Bernoulli distributions - instead of trying to directly estimate the conditional mean &lt;span class=&#34;math inline&#34;&gt;\(E(Y|X)\)&lt;/span&gt; as a linear function &lt;span class=&#34;math inline&#34;&gt;\(X\beta\)&lt;/span&gt;, it is generally more convenient to estimate a transformation of this mean, &lt;span class=&#34;math inline&#34;&gt;\(g[E(Y|X)]\)&lt;/span&gt;. This is because, for example, for a 0/1 outcome, using just the OLS will often result in fitted values outside of the [0,1] range, but using a &lt;em&gt;link function&lt;/em&gt; g that transforms the (0,1) range corresponding to the probability of success in a Bernoulli distribution into the entire real line solves this problem. If the link function is the log-odds (logit), we find ourselves looking at logistic regression. In the general case, we cannot obtain a closed form MLE solution, but by applying a first-order Taylor approximation, we obtain an algorithm that performs &lt;a href=&#34;https://web.as.uky.edu/statistics/users/pbreheny/760/S13/notes/2-19.pdf&#34;&gt;iteratively reweighted least squares (IRLS)&lt;/a&gt;, where the matrix &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; is related to the derivative of the inverse link function.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;smoothing-and-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Smoothing and regression&lt;/h2&gt;
&lt;p&gt;So far we have discussed fitting linear models between an outcome and explanatory variables, but what happens if the relationship is clearly nonlinear? One option is to add more explanatory variables, including polynomial terms, and fit a linear function to those - for example, instead of having &lt;span class=&#34;math inline&#34;&gt;\(X_1\)&lt;/span&gt; as an explanatory variable, can consider &lt;span class=&#34;math inline&#34;&gt;\(X_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(X_1^2\)&lt;/span&gt;, so that we fit the model &lt;span class=&#34;math inline&#34;&gt;\(E(Y|X_1) = \beta_0+\beta_1X_1+\beta_2X_1^2\)&lt;/span&gt; instead of &lt;span class=&#34;math inline&#34;&gt;\(E(Y|X_1) = \beta_0+\beta_1X_1\)&lt;/span&gt;. Note that the function is still linear in the explanatory variables that are considered. More flexibility can be obtained by using regression models that have piecewise polynomial terms, known as &lt;a href=&#34;https://en.wikipedia.org/wiki/Spline_(mathematics)&#34;&gt;splines&lt;/a&gt;. For the case of a single explanatory variable, another popular approach is &lt;a href=&#34;https://en.wikipedia.org/wiki/Local_regression&#34;&gt;local regression (loess)&lt;/a&gt;, which fits simple linear regression using a moving window, resulting in a smooth function. Many other advanced approaches are built on these concepts, including &lt;a href=&#34;https://en.wikipedia.org/wiki/Smoothing_spline&#34;&gt;smoothing splines&lt;/a&gt;, which place knots at every observation but have a &lt;a href=&#34;https://web.stanford.edu/class/stats202/content/lec17.pdf&#34;&gt;“roughness penalty” to reduce overfitting&lt;/a&gt; and &lt;a href=&#34;https://web.stanford.edu/class/stats202/content/lec17.pdf&#34;&gt;generalized additive models (GAMs)&lt;/a&gt; which fit smooth functions for each separate explanatory variable.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;regression-and-machine-learning&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Regression and machine learning&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Machine learning&lt;/em&gt;, also known as &lt;em&gt;statistical learning&lt;/em&gt;, focuses on “learning from the data” in order to either predict the outcomes for new data points (supervised learning) or to find patterns in the data (unsupervised learning). “Predicting” in this case means obtaining the values &lt;span class=&#34;math inline&#34;&gt;\(\hat{Y}\)&lt;/span&gt; for new points which are not in the dataset on which the model was fitted (the “training data”), using the model parameters estimated from that dataset. For supervised learning tasks, any of the regression approaches described above can be considered, noting that in machine learning, the main goal is having a low prediction error for new points, as opposed to performing statistical inference. If the number of predictors is very large, potentially larger than the number of samples, &lt;a href=&#34;https://en.wikipedia.org/wiki/Regularization_(mathematics)&#34;&gt;regularization approaches&lt;/a&gt; like ridge regression, LASSO, or elastic net can be used to reduce overfitting. Many more approaches exist, some which build on or are similar to regression. For example, &lt;a href=&#34;https://en.wikipedia.org/wiki/Support_vector_machine#Regression&#34;&gt;support vector machines (SVMs)&lt;/a&gt; for continuous outcomes are equivalent to regression models with a regularization term and the hinge loss instead of the quadratic loss function. Similarly, &lt;a href=&#34;https://github.com/greenelab/deep-review/blob/master/content/02.intro.md&#34;&gt;neural networks&lt;/a&gt; can be seen as extensions of linear or logistic regression. It is also increasingly common in machine learning to use &lt;a href=&#34;https://en.wikipedia.org/wiki/Ensemble_learning&#34;&gt;ensemble methods&lt;/a&gt; that combine several approaches and perform a type of averaging or “voting” to obtain the final predicted values.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Talbert1979高世代育种计划</title>
      <link>/post/2018-01-08-talbert1979%E9%AB%98%E4%B8%96%E4%BB%A3%E8%82%B2%E7%A7%8D%E8%AE%A1%E5%88%92/</link>
      <pubDate>Mon, 08 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-01-08-talbert1979%E9%AB%98%E4%B8%96%E4%BB%A3%E8%82%B2%E7%A7%8D%E8%AE%A1%E5%88%92/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;from Talbert &lt;span class=&#34;citation&#34;&gt;(1979)&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;摘要：&lt;/strong&gt;北卡州工业松树改良合作社成员对火炬松 &lt;em&gt;Pinus teada&lt;/em&gt; 的遗传改良已经到了第二代选择开始开花的阶段。许多来自未经改良的种植园的选择已被嫁接到&lt;strong&gt;育种种子园&lt;/strong&gt;，并将很快开花。第二代和种植园选择将很快开始。有几种育种方案可供林木育种者使用，其中，&lt;strong&gt;不连续双列&lt;/strong&gt;最适合当前的合作需求。合作社对火炬松遗传资源的有效利用将确保许多代遗传改良将带来实质性收益。通过优质种植材料生产的频繁短期改良，将实现强大的长期育种效益。。关键词：火炬松L，育种先进代。&lt;/p&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;p&gt;种子园有2种：breeding orchard和production orchard&lt;/p&gt;

&lt;/div&gt;

&lt;div class=&#34;section level1&#34;&gt;
&lt;h1&gt;引言&lt;/h1&gt;
&lt;p&gt;北卡罗来纳州立大学 - 工业松树改良合作社已经积极参与南方松树遗传改良二十多年了。重点放在火炬松（Pinus taeda L.）上。第一代选择的遗传收益是巨大的，正在通过在生产果园生产改良种子来实现。现在许多合作者正在使用改良过的种植材料，满足他们大部分的火炬松种植需求（ANONYMOUS，1978）。未来的改进将带来更大的收益。&lt;/p&gt;
&lt;p&gt;第一代火炬松选育试验已经进行了十多年。许多由合作社成员所做的第二代选择已经嫁接到生产和繁殖种子园，现在开始开花了。在任何应用的计划中，时间就是金钱。因此，第二代选择的繁殖必须尽快开始。本文提出了一个旨在最有效地利用NCSU工业松树林遗传资源的先进育种计划。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level1&#34;&gt;
&lt;h1&gt;现在合作社站在哪里&lt;/h1&gt;
&lt;p&gt;WEIR和ZOBEL（1975）详细描述了合作社所依赖的遗传基础。这里没有尝试完全审查相同的材料只会给出一个总结。 合作社的每个成员最初都负责从天然林中挑选大约30个优秀的表型。一个阶乘交配计划被用来后代测试的选择和创建一个先进一代的选择基础群体。大多数组织使用了四个或五个测试克隆，其他所有选择都与之交配。预计每个合作者将为将在一个育种克隆银行建立的每个果园计划作出约100个第二代选择。&lt;/p&gt;
&lt;p&gt;合作社的每个成员最初都负责从天然林中挑选大约30个优秀的表型。一个阶乘交配计划被用来后代测试的选择和创建一个先进一代的选择基地人口。大多数组织使用了四个或五个测试仪克隆，其他所有选择都与之交配。预计每个合作者将为将在一个育种克隆银行建立的每个果园计划作出约100个第二代选择。&lt;/p&gt;
&lt;p&gt;第一代育种主要由个体合作者进行。在后代中，育种和检测将在区域基础上进行，一个地区的所有合作者汇集资源以获得最大收益并保持广泛的遗传基础。 WEIR和ZOBEL（1975）根据地理和环境因素建立了繁殖地区，每个地区将有五到十个合作者参与育种工作。&lt;/p&gt;
&lt;p&gt;当第二代选择完成时，每个育种区将会有多达1000个选择的个体。实际上，由于许多选择中的共同祖先，人口的多样性比看起来少。个体合作者可以贡献的无关选择的数量受后代测试第一代选择中使用的测试者数量的限制。大多数繁殖地区将能够携带25至50个不相关的家庭到第二代。&lt;/p&gt;
&lt;p&gt;关于上述情况，应该指出，从第二代开始，火炬松改良活动的繁殖和生产阶段将在单独的果园进行。虽然在繁殖种群中会出现大量的选择，但是生产果园将包括更加集中的选择的个体。一个庞大而多样化的遗传基础将确保育种计划持续的长期收益，而在种子生产阶段只使用最好的选择个体。&lt;/p&gt;
&lt;p&gt;除了围绕“主线”育种计划开展的活动之外，每个合作者目前正在从未改良的种植园中找到100个新的选择。这些选择的个体将建立在繁殖克隆银行并与第二代选择同时进行测试。因此，种植材料的子代测试种群的选择将可用于第三代生产和育种计划。种植园的材料将比主线选择少一代的选择。人们认为，种植园中遇到的更加统一的环境条件会导致比主干第一代材料更高的遗传获得率，希望两个种群在合并时应具有相同的质量。计算预期的遗传增益表明情况就是如此（WEIR，1975）。当种植选择计划完成后，每个育种地区将有500到1000个新的无关选择，显着拓宽了遗传基础。&lt;/p&gt;
&lt;p&gt;目前正在进行其他几项育种和生产项目。许多合作者已经建立了由一个地区的几个合作者的最佳基因型组成的改进的第一代生产果园（通常称为1.5世代果园）。另一项正在进行的活动是培育优质的综合品种，以建立一个新的种群，从中可以建立改良的二代果园。虽然这两个计划都有望带来额外的遗传获益，但由于选择个体的后代之间的高度相关性，它们基本上是去死的。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level1&#34;&gt;
&lt;h1&gt;林木交配系统研究综述&lt;/h1&gt;
&lt;p&gt;关于林木育种的最合适的交配方案似乎没有共识。一个复杂的情况是，通常使用相同的测试来实现以下几个目标（BURDEN和SHEL BOURNE，1971）：1.一般配合能力的父母排名2.方差分量的估计.3一个基础种群的产生为下一代的选择4.演示或估计实现的遗传增益&lt;/p&gt;
&lt;p&gt;NCSU产业树改良合作社的后代的主要育种目标将是建立一个基地人口供选择。其他的目标，特别是上面的第一项，将会被强调。关于一般配合力的信息将有助于遗传基因差的父母的生产果园。&lt;/p&gt;
&lt;p&gt;创建后代测试群体的最简单和最便宜的手段是使用选定的父母的风传粉的后代。种子可以从未经选择的种群中的亲本树木或从选定基因型的克隆果园中建立的分株中收集。这样的测试通常可以很好地估计父母一般的配合能力。缺点是，如果种子是收集在未改良的立场或后代测试，花粉亲本从克隆到克隆不同，并可能密切相关;如果在果园收集种子，花粉亲本可能不是果园中的基因型随机样本。开放授粉测试对于未来几代的选择是有用的。在第一种情况下，花粉亲本是未经选择的，其次，许多选择的后代将具有相同的不可识别的花粉亲本。&lt;/p&gt;
&lt;p&gt;涉及受控授粉的简单交配系统是polycross设计，其中每个母本与多个父母的花粉混合。再次，可以获得对一般配合能力的良好估计。但是，一些花粉亲本的一般配合力良好，会导致子代间相关性显着但不明确。&lt;/p&gt;
&lt;p&gt;已经提出了多种交叉设计的几种变体（BURDON和SHELBOURNE，1971）。在嵌套的多交叉设计中，被测试的父母被分成一系列的巢穴，并且每个巢的父母与不同的花粉混合物交叉。在改良计划的生产阶段，选择不同巢穴的父母是交叉的，没有近亲交配。嵌套polymix的缺点是，需要大量的父母来创建足够数量的巢穴，并且只能在半熟的家庭中进行选择。&lt;/p&gt;
&lt;p&gt;polycross方法的另一个变体是测试仪聚合交叉设计，其中候选亲本与来源于证实的良好一般配合能力的亲本的花粉混合物杂交。这种设计的一个假设是，所使用的雄性的良好的一般配合能力将导致所选后代中花粉父母的均匀分布。树种育种者采用的另一设计是分级方案（NC状态设计I），其中不同的一个性别的父母团体交配给另一个性别的每个成员。可以获得一般和特定组合能力方差分量的估计。个体基因型的一般配合能力只能针对较少性别的成员进行估计，并且可以做出的不相关选择的数量受到用作较少性别的父母数量的限制。&lt;/p&gt;
&lt;p&gt;N.C.国家工业合作社在育种第一代选择中所采用的设计是一种因子设计（N.C. State Design II），其中四或五名男性“测试人员”父母与所有其他选择配对。对于所考虑的所有基因型，可以估计一般和特定的组合能力，因此该设计对于选择的亲本进行后代测试是很好的。随着合作社向第二代选择的转变，将会有更多的人选来创建一个新的基地人群，选择因子设计也会失去吸引力。缺点是可以选择的不相关后代的数目受限于测试者父母的数目。&lt;/p&gt;
&lt;p&gt;最完整的交配方案是双列十字。一个完整的diallel要求每个父母都与其他每个组合相交。这样的方案产生的形式将满足所有的目标，但是所需的大量交叉使得设计过于昂贵和耗时。一半排除了往复运动，往往是自我排列，产量几乎相同，成本大大降低。仍然需要大量的杂交，然而，一半的传话：很少用于主要的育种工作。已经提出了对双面书的几种修改。&lt;/p&gt;
&lt;p&gt;双排的最简化形式是由LIBBY（1968）提出的单对配合设计。通过这个计划，每个家长都与另外一个人口交配。创建的家庭是无关的。一对配对系统需要最少数量的杂交，并且每一代产生最多数量不相关的家族，因此可以保持较大的有效种群规模。选择完整的同胞家族给出了高预期的遗传增益。这种设计的缺点在于，一些优秀的家长可能会因为与劣等人交叉而使所有家长混淆一般与特定的组合能力而失去。&lt;/p&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-Talbert1979&#34;&gt;
&lt;p&gt;Talbert, JT. 1979. “An advanced-generation breeding plan for the NC State University-Industry pine tree improvement cooperative.”&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>imperative, imper=command紧急</title>
      <link>/post/2018-01-08-imperative-impercommand%E7%B4%A7%E6%80%A5/</link>
      <pubDate>Mon, 08 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-01-08-imperative-impercommand%E7%B4%A7%E6%80%A5/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Furthermore, it is &lt;strong&gt;imperative&lt;/strong&gt; that good marker sets with high exclusion power are available. (Trong et al., 2013)&lt;/li&gt;
&lt;li&gt;Given that more pine plantations are going to be established with clonal loblolly pine seedlings, it is &lt;strong&gt;imperative&lt;/strong&gt; to compare the growth and wood properties of clonally propagated loblolly pine with full-sib and open-pollinated seedlings. (Antony et al., 2014)&lt;/li&gt;
&lt;li&gt;When a tree-breeding programme proceeds beyond the first generation, early selection becomes a practical &lt;strong&gt;imperative&lt;/strong&gt;. (Burdon, 1989)&lt;/li&gt;
&lt;li&gt;And a particularly urgent &lt;strong&gt;imperative&lt;/strong&gt; is to make user-friendly software readily available to those whose crop environments are most diverse and whose food supplies are most insecure: third-world agricultural researchers. (Gauch, Piepho, Annicchiarico, 2008)&lt;/li&gt;
&lt;li&gt;Where the economic product is the seed or fruit, it is &lt;strong&gt;imperative&lt;/strong&gt; to have female and male plants in the field in an appropriate ratio. (Veatch-Blohm, 2007)&lt;/li&gt;
&lt;li&gt;To understand the full significance of epigenetic processes, however, it is &lt;strong&gt;imperative&lt;/strong&gt; to study them in an ecological context. (Bossdorf, Richards, Pigliucci, 2007)&lt;/li&gt;
&lt;li&gt;It is &lt;strong&gt;imperative&lt;/strong&gt; that our linkage model is examined in terms of its estimation precision and power. (Wu, Ma, 2005)&lt;/li&gt;
&lt;li&gt;It is &lt;strong&gt;imperative&lt;/strong&gt;, therefore, to improve the technical toolkit to such extent that cloning QTL, whether of major or minor effect, becomes a cost effective and simple routine. (Varshney, Tuberosa, 2007)&lt;/li&gt;
&lt;li&gt;To make sense of the sequence data, it is therefore &lt;strong&gt;imperative&lt;/strong&gt; that the EST sequences be cleaned, clustered and assembled to produce a minimal ‘unigene-set’. (Varshney, Tuberosa, 2007)&lt;/li&gt;
&lt;li&gt;While never much used, the persistence of convergent improvement in the literature indicates that the &lt;strong&gt;imperative&lt;/strong&gt; of inbred improvement outweighed the need for maintaining or increasing diversity. (Tracy, Chandler, 2008)&lt;/li&gt;
&lt;li&gt;The high cost of hybrid breeding relative to pure species breeding creates an &lt;strong&gt;imperative&lt;/strong&gt; to identify the most efficient breeding strategy. (Mutete, Murepa, Gapare, 2015)&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
