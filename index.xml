<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Enterprising &amp; Concentrating on Enterprising &amp; Concentrating</title>
    <link>/</link>
    <description>Recent content in Enterprising &amp; Concentrating on Enterprising &amp; Concentrating</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018 Dong</copyright>
    <lastBuildDate>Wed, 20 Apr 2016 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>latex_math</title>
      <link>/post/2018-01-29-latex_math/</link>
      <pubDate>Mon, 29 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-01-29-latex_math/</guid>
      <description>&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;符号&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;语法&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;符号&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;语法&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;符号&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;语法&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;符号&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;语法&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\leq\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;leq&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\geq\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;geq&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\neq\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;neq&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\approx\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;approx&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sim\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;sim&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\pm\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;pm&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\cdot\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;cdot&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\cdots\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;cdots&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>范数——来自知乎JI weiwei的回答</title>
      <link>/post/2018-01-29-norm-zhihu-jiweiwei/</link>
      <pubDate>Mon, 29 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-01-29-norm-zhihu-jiweiwei/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.zhihu.com/people/ji-weiwei&#34;&gt;知乎JI weiwei的回答&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;你是问向量范数还是矩阵范数？ 要更好的理解范数，就 &lt;strong&gt;要从函数、几何与矩阵的角度去理解&lt;/strong&gt;，我尽量讲的通俗一些。 我们都知道， &lt;strong&gt;函数与几何图形往往是有对应的关系&lt;/strong&gt;，这个很好想象，特别是在三维以下的空间内， &lt;strong&gt;函数是几何图像的数学概括，而几何图像是函数的高度形象化&lt;/strong&gt;，比如一个函数对应几何空间上若干点组成的图形。 但 __当函数与几何超出三维空间时，就难以获得较好的想象，于是就有了映射的概念，映射表达的就是一个集合通过某种关系转为另外一个集合。__通常数学书是先说映射，然后再讨论函数，这是因为 &lt;strong&gt;函数是映射的一个特例&lt;/strong&gt;。 __为了更好的在数学上表达这种映射关系，（这里特指线性关系）于是就引进了矩阵。__这里的矩阵就是表征上述空间映射的线性关系。而 __通过向量来表示上述映射中所说的这个集合，而我们通常所说的基，就是这个集合的最一般关系。__于是，我们可以这样理解， &lt;strong&gt;一个集合（向量），通过一种映射关系（矩阵），得到另外一个几何（另外一个向量）&lt;/strong&gt;。 那么 &lt;strong&gt;向量的范数，就是表示这个原有集合的大小&lt;/strong&gt;。 &lt;strong&gt;而矩阵的范数，就是表示这个变化过程的大小的一个度量&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;那么说到具体几几范数，其不过是定义不同，一个矩阵范数往往由一个向量范数引出，我们称之为算子范数，其物理意义都如我上述所述。&lt;/p&gt;
&lt;p&gt;以上符合知乎回答问题的方式。&lt;/p&gt;
&lt;p&gt;接下来用百度回答方式：&lt;/p&gt;
&lt;p&gt;0范数，向量中非零元素的个数。&lt;/p&gt;
&lt;p&gt;1范数，为绝对值之和。&lt;/p&gt;
&lt;p&gt;2范数，就是通常意义上的模。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>2018-01-28-English-note</title>
      <link>/post/2018-01-28-english-note/</link>
      <pubDate>Sun, 28 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-01-28-english-note/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;routine genotyping常规基因分型&lt;/li&gt;
&lt;li&gt;a wealth of很多的&lt;/li&gt;
&lt;li&gt;continues to drop&lt;/li&gt;
&lt;li&gt;appear likely to be dominant methods for the &lt;strong&gt;foreseeable&lt;/strong&gt; future&lt;/li&gt;
&lt;li&gt;emerge out of从……中出现&lt;/li&gt;
&lt;li&gt;largely主要&lt;/li&gt;
&lt;li&gt;seek to试图&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;termed&lt;/strong&gt; it simply BLUP只是称为&lt;/li&gt;
&lt;li&gt;overwhelmingly used绝大多数使用&lt;/li&gt;
&lt;li&gt;one another彼此&lt;/li&gt;
&lt;li&gt;construed解释&lt;/li&gt;
&lt;li&gt;is designed to目的是&lt;/li&gt;
&lt;li&gt;approachable summary平易近人的总结&lt;/li&gt;
&lt;li&gt;rationale基本原理&lt;/li&gt;
&lt;li&gt;in this way&lt;/li&gt;
&lt;li&gt;bogged down陷入困境&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>GS模型学习</title>
      <link>/post/2018-01-28-gs-models/</link>
      <pubDate>Sun, 28 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-01-28-gs-models/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;学习的参考文献是&lt;span class=&#34;citation&#34;&gt;(Lorenz et al. 2011)&lt;/span&gt;。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;最小二乘法不能估计全部的效应值，因为自由度不够（&lt;span class=&#34;math inline&#34;&gt;\(X&amp;#39;X\)&lt;/span&gt;矩阵奇异）。即便自由度足够用，标记间共线性会过拟合模型。过拟合模型能夸大小幅波动，导致预测能力差。&lt;/p&gt;
&lt;p&gt;于是提出各种统计模型，大致可分为4类： &lt;strong&gt;收缩、变量选择、核和降维方法&lt;/strong&gt;。 模型的基本形式一样： &lt;span class=&#34;math display&#34;&gt;\[
y_i=g(\mathbf{x}_i)+e_i
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(g(\mathbf{x}_i)\)&lt;/span&gt;是基因型和表型的关联函数。模型都设法最小化一个特定的成本函数。最小二乘的成本函数是残差的平方和：&lt;span class=&#34;math inline&#34;&gt;\(\sum e_{i}^{2}\)&lt;/span&gt;。&lt;/p&gt;
&lt;div id=&#34;blup&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;岭回归（又叫随机回归BLUP）&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
g(\mathbf{x}_i)=\sum_{k=1}^{p}x_{ik}\beta_k
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;遗传值是&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;个标记效应值之和。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\beta_i=(\mathbf{X}^{\prime}\mathbf{X}+\lambda \mathbf{I})^{-1}\mathbf{X}^{\prime}y
\]&lt;/span&gt; 与最小二乘的区别是多了一个&lt;span class=&#34;math inline&#34;&gt;\(\lambda \mathbf{I}\)&lt;/span&gt;项，引入的目的是避免矩阵&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}^{\prime}\mathbf{X}\)&lt;/span&gt;奇异，并且降低预测子间的共线性。 &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;选择的标准是最小化模型误差；还有一个方法是假定标记效应从一个均值为0的正态分布中随机抽取的，然后解混合线性模型方程，&lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;是残差方差与标记效应方差的比值var(&lt;em&gt;e&lt;/em&gt;)/var(&lt;em&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;&lt;/em&gt;)。如果var(&lt;em&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;&lt;/em&gt;)比var(&lt;em&gt;e&lt;/em&gt;)小得多的话，会造成标记效应收缩到0，&lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;很大。 岭回归的成本函数是 &lt;span class=&#34;math display&#34;&gt;\[
\frac{1}{2} \sum_i e_{i}^{2}+\frac{\lambda}{2}\mathbf{b}^{\prime}\mathbf{b}
\]&lt;/span&gt; 第二项称为约束（constraint）、惩罚（penalty）或正则化（regularizer）。&lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;越大，大的&lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;收缩得越厉害。假设是标记效应同分布，指的是同样收缩到0，不是所有效应都相等。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;lasso-least-absolute-shrinkage-and-selection-operator&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;LASSO Least absolute shrinkage and selection operator&lt;/h2&gt;
&lt;p&gt;LASSO的成本函数： &lt;span class=&#34;math display&#34;&gt;\[
\frac{1}{2} \sum_i e_{i}^{2}+\frac{\lambda}{2}\sum_k \left| \beta_k \right|
\]&lt;/span&gt; 与岭回归的共同点是都惩罚大的标记效应，二者的一般形式是： &lt;span class=&#34;math display&#34;&gt;\[
\frac{1}{2} \sum_i e_{i}^{2}+\frac{\lambda}{2}\sum_k \left| \beta_k \right|^q
\]&lt;/span&gt; &lt;em&gt;q&lt;/em&gt;=1时是LASSO，&lt;em&gt;q&lt;/em&gt;=2时是RR-BLUP，&lt;span class=&#34;math inline&#34;&gt;\(1\leq q\leq 2\)&lt;/span&gt;时，称为弹性网回归（elastic net）。因为正则化矩阵公式不同，LASSO的系数收缩比RR-BLUP的要强，而且一些系数收缩到0，产生一个稀疏模型，因此LASSO也能进行变量选择（所以有selection operator的意思）。LASSO的一个问题也是&lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;的取值问题。 Park and Casella (2008)提出贝叶斯LASSO。 在非贝叶斯LASSO中，解决方案允许有n - 1个非零回归系数。 这是一个问题，因为在密集的标记数据的情况下，没有理由为什么训练集中的个体数量应该限制具有非零效应的标记的数量。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;reproducing-kernel-hilbert-spaces-rkhs-and-support-vector-machine-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Reproducing kernel Hilbert spaces (RKHS) and support vector machine regression&lt;/h2&gt;
&lt;p&gt;RKHS：经典加性遗传模型结合核函数。核函数将预测变量转换为观测值之间的一组距离，以产生一个&lt;span class=&#34;math inline&#34;&gt;\(n\times n\)&lt;/span&gt;正定矩阵，用于线性模型。 __输入空间和特征空间__是这个文献中经常遇到的术语。 在GS的情况下， &lt;strong&gt;输入数据是标记得分数据&lt;/strong&gt;，因此，输入空间是多维空间，其中每个个体的位置由其标记分数确定。 通过 &lt;strong&gt;对输入数据应用内核函数将输入空间转换为特征空间&lt;/strong&gt;。 RKHS可以用矩阵符号表示： &lt;span class=&#34;math display&#34;&gt;\[
g(x)=\mathbf{K}_h\mathbf{\alpha}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{K}_h\)&lt;/span&gt;是核心条目的矩阵，量化个体彼此之间的距离，就像量化系谱关系的加性关系矩阵，即个体之间的遗传距离。这里的区别在于，核函数包括一个或多个平滑参数（ &lt;strong&gt;h&lt;/strong&gt;）以影响特征空间中的距离和输入空间中的距离之间的关系。&lt;span class=&#34;math inline&#34;&gt;\(n\times 1\)&lt;/span&gt;向量&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;可以被解释为可以使用标准混合模型方程估计的特征空间内的单独效应。CV或bootstrap方法可以用来确定h的最优值，例如在群体和性状之间的模型中允许很大的灵活性。RKHS方法可以在标准的定量遗传模型框架内解释。 由于内核方法包含了很大的灵活性，而且没有线性假设，所以它们可以 &lt;strong&gt;同时捕获所有类型的非加性效应&lt;/strong&gt;。 &lt;strong&gt;支持向量机也是核方法。&lt;strong&gt;其成本函数可称为 &lt;/strong&gt;&lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt;-intensive&lt;/strong&gt;&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[
C \sum_i L_{\varepsilon}(e_{i})+\frac{1}{2}\mathbf{b}^{\prime}\mathbf{b}
\]&lt;/span&gt; C是一个用来衡量误差对成本函数的贡献的常数；&lt;span class=&#34;math inline&#34;&gt;\(L_{\varepsilon}\)&lt;/span&gt;是 &lt;strong&gt;&lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt;-intensive损失函数&lt;/strong&gt;，意思是不惩罚误差，除非它们大于&lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt;： $$ L_{}(e_{i}) =
&lt;span class=&#34;math display&#34;&gt;\[\begin{cases}
 0, &amp;amp; \text{for} \left| e_{i} \right|&amp;lt;\varepsilon \\
 
 \left| e_{i} \right|-\varepsilon, &amp;amp; \text{otherwise}
 \end{cases}\]&lt;/span&gt;
&lt;p&gt;$$ 正常情况下，残差的总和不管是大还是小，都会对回归模型进行惩罚。在这里，只有最小化“大”（&amp;gt; &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt;）残差的模型才是有利的，而所有其他残差是可以接受的。 &lt;strong&gt;这个步骤的目的是开发牺牲许多小错误为了排除大错误的模型。&lt;/strong&gt; 这个问题的解决方案涉及 &lt;strong&gt;拉格朗日（Lagrange）优化&lt;/strong&gt;，其产生一组系数，乘以训练样本与正在预测的观察之间的所有可能的内积的和。训练样本 __i__和 __j__的内积定义为&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{x}_i^{\prime}\mathbf{x}_j\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{x}\)&lt;/span&gt;是观测标记得分向量。为了允许非线性，内积项可以被核函数替换，有效地将SVR优化从输入空间传送到特征空间。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;partial-least-squares-regression-and-principle-component-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Partial least squares regression and principle component regression降维&lt;/h2&gt;
&lt;p&gt;从变量中提取 __潜变量__为预测变量的线性组合，并用于响应预测： &lt;span class=&#34;math display&#34;&gt;\[
g(\mathbf{x}_i)=\sum_{l=1}^{w}t_{il}\beta_l
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(t_{il}\)&lt;/span&gt;是从原始&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;预测变量中提取的潜在变量，&lt;span class=&#34;math inline&#34;&gt;\(\beta_l\)&lt;/span&gt;是与该变量相关联的效应。即&lt;span class=&#34;math inline&#34;&gt;\(t_{il}=\mathbf{x}_i\mathbf{u}_l\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{u}_l\)&lt;/span&gt;是一个&lt;span class=&#34;math inline&#34;&gt;\(p\times1\)&lt;/span&gt;的向量，用于在线性组合中给潜变量&lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt;加权。通常&lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt;是潜变量的个数，比&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;要小得多，&lt;span class=&#34;math inline&#34;&gt;\(t_{il}\)&lt;/span&gt;是正交的，目的是消除共线性的问题。这个程序涵盖了各种技术，取决于哪个变异来源被认为是最重要的。在PC回归中，选择潜在变量尽可能多地解释原始预测变量。这种方法产生的预测空间信息向量可能不会与响应的变化相关联。在PLS中，选择潜变量以使潜变量和响应之间的关系尽可能强。 __为了避免由于太少或太多潜在变量而导致模型准确性降低（Solberg等人，2009），可以通过CV或相关技术来确定潜在变量的数量。__在PCR和PLS中，&lt;span class=&#34;math inline&#34;&gt;\(g(\mathbf{x}_i)\)&lt;/span&gt;都可以重新写成预测变量的直接函数，不用潜变量： &lt;span class=&#34;math display&#34;&gt;\[
\beta_k^*=\sum_{l=1}^{w}\beta_lu_{lk}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;贝叶斯&lt;/h2&gt;
&lt;p&gt;RR-BLUP所做的遗传效应在基因组上均匀分布的假设并不令人满意，Meuwissen等 （2001）试图用贝叶斯分析来放松它。&lt;/p&gt;
&lt;div id=&#34;bayesa&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;BayesA&lt;/h3&gt;
&lt;p&gt;每个标记效应&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;是从具有其自身方差的正态分布中抽取的：&lt;span class=&#34;math inline&#34;&gt;\(N(0,var(\beta_k))\)&lt;/span&gt;，这使得每个标记都可以向不同的程度缩小到零。反过来，方差参数从经过缩放的反向&lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt;分布中采样。这个模型也被称为贝叶斯收缩回归。 ### BayesB 没有效应的标记也给了概率。 如果遗传变异存在于少数基因座而在许多基因座缺失，这个 &lt;strong&gt;模型将更好地反映潜在的遗传架构&lt;/strong&gt;。 &lt;span class=&#34;math display&#34;&gt;\[
g(\mathbf{x}_i)=\sum_{k=1}^{p}x_{ik}\beta_k\gamma_k
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\gamma_k\)&lt;/span&gt;是指示预测模型中标记 _k_的存在的指示变量。&lt;span class=&#34;math inline&#34;&gt;\(\beta_k\)&lt;/span&gt;服从均值为0、有限方差的正态分布。&lt;span class=&#34;math inline&#34;&gt;\(\beta_k\)&lt;/span&gt;方差的先验分布服从一个混合分布： &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
 var(\beta_k)=0,\quad &amp;amp;\text{with probability} \pi \\
 var(\beta_k)\sim \chi^{-2}(\mathbf{v},\mathbf{S}), \quad&amp;amp; \text{with probability} (1-\pi) \\
 \end{align}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\pi=0\)&lt;/span&gt;时，BayesB和BayesA是一样的。然而，在处理真实的生物体和性状时，这个参数（&lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt;）的合理值是未知的。为了克服这个问题，一个称为 __BayesC&lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt;__的分析本身估计&lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt;，其中&lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt;的先验分布在0和1之间是均匀的。另外， __BayesC&lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt;__假设所有效应&lt;span class=&#34;math inline&#34;&gt;\(\gamma_k=1\)&lt;/span&gt;的标记的先验方差相等。即&lt;span class=&#34;math inline&#34;&gt;\(\gamma_k=0\)&lt;/span&gt;时&lt;span class=&#34;math inline&#34;&gt;\(\beta_k=0\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(\gamma_k=1\)&lt;/span&gt;时&lt;span class=&#34;math inline&#34;&gt;\(\beta_k\sim N(0,\hat{\sigma}_\beta^2)\)&lt;/span&gt;。反过来，该方法一起估计所有非零的&lt;span class=&#34;math inline&#34;&gt;\(\hat{\sigma}_\beta^2\)&lt;/span&gt;。在估计&lt;span class=&#34;math inline&#34;&gt;\(\hat{\sigma}_\beta^2\)&lt;/span&gt;时，以这样的方式对标记进行分组会对所有先验进行加权。 &lt;strong&gt;随机搜索变量选择（SSVS）&lt;/strong&gt;，早于Meuwissen的（2001）BayesB，但是与BayesB有大致相同的先验假设。&lt;/p&gt;
&lt;p&gt;__所有这些贝叶斯模型共同的问题是参数估计不能以分析方式获得。__然而，有可能开发所谓的马尔可夫链蒙特卡罗（MCMC）算法，其从后验分布中对参数值进行采样。 然后可以通过重复抽样和计算适当的汇总统计数据（例如分布的均值或中值）来近似这些分布。 不幸的是，这个解决方案在计算上非常昂贵。 因此， __非MCMC方法__已经被用于估计这些汇总统计。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;统计方法总结&lt;/h2&gt;
&lt;p&gt;在RR-BLUP模型下，标记效应是来自具有固定方差的正态分布的样本。通过缩小所有的标记效应到相同的程度，并包括模型中的所有标记，RR-BLUP的使用意味着从业者相信这个性状被许多具有小影响的基因座所控制。相反，贝叶斯B假设大多数位点对性状没有影响，因此大多数标记都不在预测模型中。包含在模型中的标记具有从具有不同方差的分布中采样的效果。因此，贝叶斯B有效地假定，性状是由相对较少的效应大小不同的基因座控制的。在BayesC&lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt;的情况下，包含在模型中的标记的比例是根据数据估计的。包括在模型中的标记效应是从估计方差的相同分布从数据。这使得BayesC&lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt;更具灵活性来模拟寡源到多基因的性状。 我们强调这些模型的先验和它们的含义，强调这些模型假定不同的基因结构。因此，就遗传结构不同而言，所有性状和种群都没有单一的最佳模型。 &lt;strong&gt;如果遗传结构由多个小效应位点组成（Buckler et al。，2009），RR-BLUP和利用全球遗传关系信息的模型运作良好。如果大效QTL解释了很多遗传变异（Anderson等，2001; Munkvold等，2009），贝叶斯B等模型应该受到青睐。此外，这些示例模型假定可加性，并且如果非加性遗传效应是重要的，则可能不会很好地执行。一些人群中存在流行的非加性效应的证据（Dudley and Johnson，2009）; RKHS和SVR在这些情况下可能占有优势。&lt;/strong&gt;&lt;/p&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-Lorenz2011&#34;&gt;
&lt;p&gt;Lorenz, Aaron J., Shiaoman Chao, Franco G. Asoro, Elliot L. Heffner, Takeshi Hayashi, Hiroyoshi Iwata, Kevin P. Smith, Mark E. Sorrells, and Jean-Luc Jannink. 2011. “Genomic Selection in Plant Breeding.” In &lt;em&gt;Advances in Agronomy&lt;/em&gt;, 110:77–123. C. doi:&lt;a href=&#34;https://doi.org/10.1016/B978-0-12-385531-2.00002-5&#34;&gt;10.1016/B978-0-12-385531-2.00002-5&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>2018-01-26-English-note</title>
      <link>/post/2018-01-26-english-note/</link>
      <pubDate>Fri, 26 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-01-26-english-note/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Join Fisher Scientific and various other &lt;strong&gt;vendors&lt;/strong&gt; 供应商 on Monday, January 29 from 9:30 am - 11:30 am &lt;strong&gt;in the 3rd-floor&lt;/strong&gt; lobby of Thomas Hall. Come check out all of the latest new products and &lt;strong&gt;promotions&lt;/strong&gt; 促销 available!&lt;/li&gt;
&lt;li&gt;appalled震惊的&lt;/li&gt;
&lt;li&gt;resume重新开始&lt;/li&gt;
&lt;li&gt;excerpts节选&lt;/li&gt;
&lt;li&gt;give rise to引起&lt;/li&gt;
&lt;li&gt;detrimental to有害的，不利于&lt;/li&gt;
&lt;li&gt;Testable hypotheses&lt;/li&gt;
&lt;li&gt;have been reluctant to&lt;/li&gt;
&lt;li&gt;owing to&lt;/li&gt;
&lt;li&gt;nondestructive techniques&lt;/li&gt;
&lt;li&gt;calibration models&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;can efficiently be&lt;/strong&gt; used for screening&lt;/li&gt;
&lt;li&gt;surrogate替代的&lt;/li&gt;
&lt;li&gt;save considerable resources&lt;/li&gt;
&lt;li&gt;growth and yield&lt;/li&gt;
&lt;li&gt;have been ongoing for over 50 years&lt;/li&gt;
&lt;li&gt;exceed&lt;/li&gt;
&lt;li&gt;intensive management集约化经营&lt;/li&gt;
&lt;li&gt;it is recognized that……是公认的&lt;/li&gt;
&lt;li&gt;undertaken&lt;/li&gt;
&lt;li&gt;examining wood properties&lt;/li&gt;
&lt;li&gt;quality-related properties&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Several options&lt;/strong&gt; now &lt;strong&gt;exist for&lt;/strong&gt; the nondestructive assessment&lt;/li&gt;
&lt;li&gt;determined nondestructively&lt;/li&gt;
&lt;li&gt;have yet to be尚未&lt;/li&gt;
&lt;li&gt;a valuable tool&lt;/li&gt;
&lt;li&gt;at an impasse陷入僵局&lt;/li&gt;
&lt;li&gt;in any event无论如何&lt;/li&gt;
&lt;li&gt;This practice &lt;strong&gt;nevertheless&lt;/strong&gt; retains the disadvantage of biased effect estimates&lt;/li&gt;
&lt;li&gt;quandary困境&lt;/li&gt;
&lt;li&gt;capitalizing on利用&lt;/li&gt;
&lt;li&gt;on the strength of基于&lt;/li&gt;
&lt;li&gt;in the same way同样地&lt;/li&gt;
&lt;li&gt;be analogous to类似于&lt;/li&gt;
&lt;li&gt;this condition does not hold这种情况不成立&lt;/li&gt;
&lt;li&gt;fall short of达不到&lt;/li&gt;
&lt;li&gt;outpace超过……的速度&lt;/li&gt;
&lt;li&gt;be referred to as被称为&lt;/li&gt;
&lt;li&gt;countless students无数&lt;/li&gt;
&lt;li&gt;counter-intuitive反直觉的&lt;/li&gt;
&lt;li&gt;to a first approximation大致上&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>2018-01-26-Vergara2004-realized-gains</title>
      <link>/post/2018-01-26-vergara2004-realized-gains/</link>
      <pubDate>Fri, 26 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-01-26-vergara2004-realized-gains/</guid>
      <description>&lt;p&gt;摘要：利用美国东南部大型矩形地块种植的38个田间试验数据，估算了第一代湿地松湿地松（松材线虫湿地松）的选育成果。这些试验由佛罗里达大学合作森林遗传学研究计划（19项试验）和佐治亚大学植物管理研究合作社（19项试验）种植的材料选择用于量的增长。所有的试验都包含从不受污染或轻微流淌的第一代种子园采集的湿地松籽苗。方差分析显示，抗锈性（I50 = 43.1％）和场地指数（4.3％）平均上升幅度较小，单株树体积（7.7％）和林分产量（10.2％）均有显着增加。通过实现收益相互作用的营养处理和年龄从来没有显着，但种子和试验之间的显着相互作用表明实现的收益在所有地点都不一致。总体结果与预测的抗锈性育种值高度一致，但低于预期的体积。平均而言，第一代材料的实际增产率约为10％，或额外的树皮体积为25立方米。 （357英尺3英寸）25年。&lt;/p&gt;
&lt;hr /&gt;
&lt;div class=&#34;section level1&#34;&gt;
&lt;h1&gt;介绍&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;实现遗传增益是证明遗传改良投资合理化和量化木材生产和经济效益进步的最佳方法&lt;/strong&gt;（Zobel and Talbert 1984; Eldridge et al。1994）。事实上，在最近的森林遗传和树木育种史中，实现收益的提供一直是人们关注的主要问题之一（Kanowski，1993），即使知道大规模的树木育种计划，少量的遗传增益也足以证明投入的资源（Zobel和Talbert 1984）。 &lt;strong&gt;将不同类型的改良种子（例如开放授粉的家系，控制授粉的家系，混合家系，种子园混合）与设计用于代表未经改进的物质的对照进行比较，从而量化所实现的遗传增益&lt;/strong&gt;。研究人员通过使用小地块（单树地块和行地块）或大矩形地块来估计实现的收益。 &lt;strong&gt;小地块的优点是允许包含许多家系，而不会过度增加复制大小，从而提供更高的统计精度&lt;/strong&gt;。然而，对于生长性状， &lt;strong&gt;如果优势表型由于夸大的边缘效应而获得了较早的竞争优势，那么小地块就可以提供有偏差的估计值&lt;/strong&gt;，其中种子群体主要影响邻居的表现（Foster 1989; Lambeth et al。1994）。因此， &lt;strong&gt;利用高度或茎干等性状在小块土地上单独表现树木性能的估计遗传增益可能高估了最好的家庭，而低估了最差的家庭&lt;/strong&gt;（Rockwood 2000）。 大的矩形图可以避免不准确的估计，因为给定绘图中的所有树都来自同一个家系。这些地块允许每个种田使用作为种子之间的缓冲区的图中的外部树来表达其遗传潜力。这样， &lt;strong&gt;只有内在的树被分析，结果并没有受到家系竞争的偏见&lt;/strong&gt;（Zobel and Talbert 1984; Lowerts 1986; Williams and Matheson 1994）。 &lt;strong&gt;在树冠关闭之后，相邻地块之间光线的竞争将变得重要，使得大地块对于长期试验尤其有价值&lt;/strong&gt;（White，1987; Foster，1992; Williams和Matheson，1994）。事实上，大型矩形地块应该与现场部署密切相关。即使是相对较少的遗传条目，但是，大的矩形图需要大的复制大小，从而降低统计精度。因此，需要大量的试验场来确定统计学显着性差异，并精确估计遗传增益的大小（Dhakal等，1996）。总之， &lt;strong&gt;当目标是以单位面积为基础估计生长性状的实际遗传增益时，最好的选择是在许多地点种植大块土地&lt;/strong&gt;（Dhakal等1996; Zobel和Talbert 1984）。 很少有研究使用在矩形区域建立的试验（Dhakal 1995）从多个地点获得遗传增益，因为实施和维持这些试验的成本很高，以及可靠的结果需要很长的时间间隔。此外，获取信息时，适用于通常不在计划中的遗传物质，使得许多公司对这些研究不具吸引力。像佛罗里达大学的合作性森林遗传研究计划（CFGRP）这样的大型合作努力具有在多个地点使用大地块进行实现的遗传增益试验的结构。然而，用这种方法估计实现的收益在这项研究之前还没有在湿地松（Pinus elliottii Engelm。var。elliottii）中进行，因为现在只有足够的数据才能获得高的统计精度。 在文献中可以找到 &lt;strong&gt;许多使用行小区的实现增益研究&lt;/strong&gt;。这些包括火炬松（Pinus taeda L.）（La Farge 1993; Lambeth 2000; Li等2000），阿勒颇松（Pinus halepensis Mill。）（Matziris 2000），短叶松（Pinus echinata Mill。），白松（Pinus strobus L.）（La Farge 1993）和斜松（Dhakal等，1996）。然而， &lt;strong&gt;很少有研究可用于大型矩形地块&lt;/strong&gt;。黑松（Picea mariana（Mill。）BSP）和千斤顶松（Jack pine（Pillsa mariana（Mill。）BSP）获得的最广泛的信息是辐射松（Pinus radiata D.Don）（Eldridge 1982; Carson等1999a，1999b） Pinus banksiana Lamb。）（Simpson和Kathleen 1997）。 目前的研究是第一次尝试评估实际遗传增益的斜线松，使用许多试验，每个组成的大型矩形地块。具体目标是（i）准确估计增长，抗锈性和林分产量的实际收益; （ii）估计这些性状在几个年龄段实现的遗传获益; （3）评估营林，生锈危害和现场指标对实现收益的影响。&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>2018-01-24-English-note</title>
      <link>/post/2018-01-24-english-note/</link>
      <pubDate>Wed, 24 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-01-24-english-note/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;urged to consult&lt;/li&gt;
&lt;li&gt;poorly adapted to适应性差&lt;/li&gt;
&lt;li&gt;are urged to consult鼓励&lt;/li&gt;
&lt;li&gt;In &lt;strong&gt;dichotomous&lt;/strong&gt; traits, logistic regression has been used to conduct association tests for GWAS&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;proneness&lt;/strong&gt; to overfitting&lt;/li&gt;
&lt;li&gt;these &lt;strong&gt;defects&lt;/strong&gt; can be addressed&lt;/li&gt;
&lt;li&gt;offers many avenues to&lt;/li&gt;
&lt;li&gt;In principle大体上&lt;/li&gt;
&lt;li&gt;serve to用来&lt;/li&gt;
&lt;li&gt;vegetative propagules营养繁殖体&lt;/li&gt;
&lt;li&gt;tissue culture plantlets组培苗&lt;/li&gt;
&lt;li&gt;for the purposes of为了…目的&lt;/li&gt;
&lt;li&gt;where appropriate在适当的时候&lt;/li&gt;
&lt;li&gt;weigh in参与&lt;/li&gt;
&lt;li&gt;consult with咨询&lt;/li&gt;
&lt;li&gt;Other effects &lt;strong&gt;fitted to&lt;/strong&gt; the design model&lt;/li&gt;
&lt;li&gt;some of the &lt;strong&gt;characteristic&lt;/strong&gt; design and analysis problems&lt;/li&gt;
&lt;li&gt;in either case不论发生何种情况&lt;/li&gt;
&lt;li&gt;explicitly defined&lt;/li&gt;
&lt;li&gt;alternative route备选路线&lt;/li&gt;
&lt;li&gt;or else否则&lt;/li&gt;
&lt;li&gt;an integrated system集成系统&lt;/li&gt;
&lt;li&gt;widely accepted普遍认可&lt;/li&gt;
&lt;li&gt;satisfy……needs&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;mobilize resources&lt;/strong&gt; to support implementation of动员资源以支持&lt;/li&gt;
&lt;li&gt;comprised&lt;/li&gt;
&lt;li&gt;incorporate&lt;/li&gt;
&lt;li&gt;is advantageous over&lt;/li&gt;
&lt;li&gt;stuck with无法摆脱&lt;/li&gt;
&lt;li&gt;In this regard,就这一点而言&lt;/li&gt;
&lt;li&gt;interrelationships&lt;/li&gt;
&lt;li&gt;be very efficient at非常有效&lt;/li&gt;
&lt;li&gt;care must be taken to必须小心&lt;/li&gt;
&lt;li&gt;be analogous to类似于&lt;/li&gt;
&lt;li&gt;pass on to传给&lt;/li&gt;
&lt;li&gt;in practice&lt;/li&gt;
&lt;li&gt;by virtue of由于，凭借&lt;/li&gt;
&lt;li&gt;eligible合格的&lt;/li&gt;
&lt;li&gt;on a &lt;strong&gt;staggering&lt;/strong&gt; number of&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Individual perceptions&lt;/strong&gt; of what &lt;strong&gt;is constituted by&lt;/strong&gt; bioinformatics __vary widely__个人看法&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Cantor2010 GWAS中的统计方法</title>
      <link>/post/2018-01-24-cantor2010/</link>
      <pubDate>Wed, 24 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-01-24-cantor2010/</guid>
      <description>&lt;div class=&#34;section level1&#34;&gt;
&lt;h1&gt;统计方法&lt;/h1&gt;
&lt;p&gt;交互建模在统计学方面有着悠久的历史，在这篇综述中不可能公正地讨论这个话题。要求更广泛的报道的读者可以参考两篇极好的综合评论文章[55,56]。这里我们强调数据挖掘方面的一些最新进展，可以用来优先考虑GWAS结果。在二分性状中，逻辑回归已经被用来进行GWAS的关联测试;比例相等或费舍尔精确测试的替代测试缺乏处理多个预测因子的灵活性。对于连续性状，GWAS使用线性回归。 &lt;strong&gt;回归的一个主要力量是它很容易提供一个包含交互的机会&lt;/strong&gt;。回归分析的其他优点包括（1）明确的参数模型; （2）参数估计的稳定算法; （3）主效应和相互作用效应的似然比和F检验的可用性; （4）容易并入诸如年龄，性别和族裔的协变量;和（5）可靠和有据可查的软件的广泛可用性。&lt;/p&gt;
&lt;p&gt;大部分的缺点与当前数据集的规模和预测者超过观测值有关。其缺点是：（1）数量性状的正态性假设失败;（2）p值背后大样本近似的细分;（3）欠定问题中的搜索算法失败;（4）倾向于过拟合;（5）提供备用解决方案，以及（6）在检测交互之前需要检测主效应的模型选择的分层性质。在解释如何处理这些缺陷之前，简要总结一些检测上位性的替代方法是有用的。&lt;/p&gt;
&lt;p&gt;数据挖掘的新兴领域57为理解交互提供了许多途径。当然， &lt;strong&gt;逻辑回归和判别分析是密切相关的&lt;/strong&gt;。原则上，任何判别分析方法都可以将案件与控制分开。 __CART（分类和回归树）和随机森林__等判别分析方法与交互建模显然是相关的。其他更有针对性的竞争者包括多 &lt;strong&gt;因子降维方法，组合分割方法62和限制分割方法&lt;/strong&gt;。尽管这些工具在探索性数据分析中有所帮助，而且在区分病例和控制方面也很出色，但是 &lt;strong&gt;它们受到了一些限制。例如，纯组合方法不产生效应量或p值，也不能处理协变量。根据所采用的算法，它们很容易被大量的预测因子和交叉验证和置换测试的要求所淹没&lt;/strong&gt;。针对这些批评，一些方法正在重新设计。例如，Lou等[52]修改了多因素降维方法64，以便进行协变量调整，连续和二元特征分析以及系谱数据。他们的新公式重视FBAT（家庭联想测试）方法的智力负担[65,66]。其他创新包括引入交互的熵和条件熵度量67,68;利用个人之间的邻近措施69？1;神经网络的应用，72遗传规划，73逻辑回归，74模式挖掘，64,75和贝叶斯分区76。&lt;/p&gt;
&lt;p&gt;客观评估这一令人眼花缭乱的方法的需求是显而易见的。 Cordell55强调处理大量SNP数据集时计算速度的重要性。实际上，统计方法和软件是密不可分的。在计算速度的基础上，Cordell更喜欢使用标准回归，随机森林和贝叶斯分区的 &lt;strong&gt;PLINK&lt;/strong&gt;，77 Random Jungle，78和BEAM等程序。 Musani及其同事[56]在他们的建议中不太具体，并且认为综合的方法可以为消费者提供最好的服务。在我们自己的研究中，我们主要 &lt;strong&gt;使用PLINK77进行经典回归，Mendel79进行套索惩罚回归&lt;/strong&gt;。80？2 PLINK是用户友好的，具有内置的数据管理和质量控制例程。孟德尔诊断较少，但 &lt;strong&gt;在SNP数量远远超过个体数量时，在模型选择上更胜一筹&lt;/strong&gt;。在描述对传统参数模型的改进之前，让我们简单地提一些研究设计问题。在仅有情况的设计中，我们寻找标记基因型的不依赖性。在偏离独立性的可能测试中，值得挑出 &lt;strong&gt;卡方检验，熵和最大Z分数&lt;/strong&gt;。这些测试具有相当大的力量，但应该记住它们的局限性。 因为 &lt;strong&gt;他们不允许协变量，他们容易受到人口分层和连锁不平衡的影响&lt;/strong&gt;。 __潜在的应急表可能是稀疏的，所以p值的置换评估是一个好主__意。 不幸的是， &lt;strong&gt;排列测试的代价是更重的计算&lt;/strong&gt;。 另一方面， &lt;strong&gt;谱系数据也带来了真正的挑战。 除非完全由连锁不平衡驱动，否则连锁分析需要系谱。&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>2018-01-23-English-note</title>
      <link>/post/2018-01-23-english-note/</link>
      <pubDate>Tue, 23 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-01-23-english-note/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;be central to&lt;/li&gt;
&lt;li&gt;be cast as&lt;/li&gt;
&lt;li&gt;convex凸面&lt;/li&gt;
&lt;li&gt;psyched激动的兴奋的&lt;/li&gt;
&lt;li&gt;main grant主要资助&lt;/li&gt;
&lt;li&gt;under-used较少使用&lt;/li&gt;
&lt;li&gt;vehemently opposed强烈反对&lt;/li&gt;
&lt;li&gt;Gauging测量&lt;/li&gt;
&lt;li&gt;Raisin葡萄干&lt;/li&gt;
&lt;li&gt;neural network神经网络&lt;/li&gt;
&lt;li&gt;This study is a sequel to the reports of是某研究的后续&lt;/li&gt;
&lt;li&gt;competing model如果同时有多个模型候选，那么彼此是竞争模型&lt;/li&gt;
&lt;li&gt;is appropriate for；is also capable of适于&lt;/li&gt;
&lt;li&gt;expensive and tme-consuming to collect 收集起来耗时费力&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;highly limited&lt;/strong&gt; in availability 非常有限&lt;/li&gt;
&lt;li&gt;are essental for 重要的&lt;/li&gt;
&lt;li&gt;epidemiology&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Producing models&lt;/strong&gt; that can predict abundance &lt;strong&gt;well&lt;/strong&gt;, with good resolution over large areas, has &lt;strong&gt;therefore&lt;/strong&gt; been &lt;strong&gt;an important aim&lt;/strong&gt; in ecology, but &lt;strong&gt;poses&lt;/strong&gt; considerable challenges. 带来极大的挑战&lt;/li&gt;
&lt;li&gt;have driven an explosion in the use of these techniques&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;none of these has produced consistently satisfactory results and each has signifcant theoretcal or practcal limitatons&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;with very low precision in predictons&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;none have yet performed well enough to be of use for&lt;/strong&gt; many real-world applicatons&lt;/li&gt;
&lt;li&gt;envisage设想&lt;/li&gt;
&lt;li&gt;are a signifcant improvement on previously widely available distributon data&lt;/li&gt;
&lt;li&gt;custom-writen定制编写&lt;/li&gt;
&lt;li&gt;burgeoning adj. 增长迅速的；生机勃勃的&lt;/li&gt;
&lt;li&gt;a standard method&lt;/li&gt;
&lt;li&gt;A &lt;strong&gt;substantial&lt;/strong&gt; number of&lt;/li&gt;
&lt;li&gt;in aggregate with联合&lt;/li&gt;
&lt;li&gt;A controversy &lt;strong&gt;ensued&lt;/strong&gt; as to whether much of this risk could be explained随之而来的&lt;/li&gt;
&lt;li&gt;be likely to&lt;/li&gt;
&lt;li&gt;persecution迫害&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Approximately&lt;/strong&gt; half of the book is written, and all completed chapters are now available as online preview.&lt;/li&gt;
&lt;li&gt;the same issues &lt;strong&gt;arise&lt;/strong&gt; over and over&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;I have attempted to&lt;/strong&gt; collect &lt;strong&gt;my accumulated knowledge&lt;/strong&gt; from these &lt;strong&gt;interactions&lt;/strong&gt; &lt;strong&gt;in the form of&lt;/strong&gt; this book.&lt;/li&gt;
&lt;li&gt;As of this writing截至撰写本文&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;please also feel free to&lt;/strong&gt; record these suggestions as issues on github&lt;/li&gt;
&lt;li&gt;with no &lt;strong&gt;manual&lt;/strong&gt; post-processing手动&lt;/li&gt;
&lt;li&gt;brush up擦亮，提高&lt;/li&gt;
&lt;li&gt;We are also grateful to&lt;/li&gt;
&lt;li&gt;prime minister&lt;/li&gt;
&lt;li&gt;humiliating defeat丧权辱国&lt;/li&gt;
&lt;li&gt;take the plunge冒险尝试；采取决定性步骤&lt;/li&gt;
&lt;li&gt;is contingent upon视…而定&lt;/li&gt;
&lt;li&gt;predisposing variants = causal variants&lt;/li&gt;
&lt;li&gt;substantive information实质性信息&lt;/li&gt;
&lt;li&gt;the specific accomplishments特定的成就&lt;/li&gt;
&lt;li&gt;productive collaboration高效的协作&lt;/li&gt;
&lt;li&gt;In many cases&lt;/li&gt;
&lt;li&gt;discrete&lt;/li&gt;
&lt;li&gt;detectable&lt;/li&gt;
&lt;li&gt;significantly associated SNPs&lt;/li&gt;
&lt;li&gt;raise the risk by&lt;/li&gt;
&lt;li&gt;be effective in elucidating&lt;/li&gt;
&lt;li&gt;placed in&lt;/li&gt;
&lt;li&gt;easily accessible databases&lt;/li&gt;
&lt;li&gt;increasingly affordable costs&lt;/li&gt;
&lt;li&gt;in the context of&lt;/li&gt;
&lt;li&gt;analytic challenges&lt;/li&gt;
&lt;li&gt;most critical&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;thorny&lt;/strong&gt; problem&lt;/li&gt;
&lt;li&gt;Although this was problematic&lt;/li&gt;
&lt;li&gt;viewed it as a potential strength&lt;/li&gt;
&lt;li&gt;sparked&lt;/li&gt;
&lt;li&gt;the creation of appropriate software&lt;/li&gt;
&lt;li&gt;it was anticipated&lt;/li&gt;
&lt;li&gt;follow-up studies随访研究&lt;/li&gt;
&lt;li&gt;Saccone &lt;strong&gt;and colleagues&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;evidence supporting the &lt;strong&gt;biological relevance&lt;/strong&gt; of each associated SNP&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;implicated&lt;/strong&gt; pathway&lt;/li&gt;
&lt;li&gt;even if they are &lt;strong&gt;considered in aggregate&lt;/strong&gt; 即使它们被 &lt;strong&gt;综合考虑&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;effect sizes&lt;/li&gt;
&lt;li&gt;appear in&lt;/li&gt;
&lt;li&gt;once __prohibitively expensive__一度非常昂贵&lt;/li&gt;
&lt;li&gt;genetic etiology病理学&lt;/li&gt;
&lt;li&gt;a small fraction of&lt;/li&gt;
&lt;li&gt;remains to be investigated&lt;/li&gt;
&lt;li&gt;go overlooked&lt;/li&gt;
&lt;li&gt;focused primarily on&lt;/li&gt;
&lt;li&gt;met their expectations&lt;/li&gt;
&lt;li&gt;these &lt;strong&gt;salient&lt;/strong&gt; 突出的 publications &lt;strong&gt;each&lt;/strong&gt; provide some support for the position that GWAS are only the first step in the gene identification process&lt;/li&gt;
&lt;li&gt;More specifically更具体地说&lt;/li&gt;
&lt;li&gt;is comparable to可相提并论&lt;/li&gt;
&lt;li&gt;put the edges and corners in place&lt;/li&gt;
&lt;li&gt;advocate&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;providing support for&lt;/strong&gt; applying the second method&lt;/li&gt;
&lt;li&gt;make progress in在…方面取得进步，取得进展&lt;/li&gt;
&lt;li&gt;impractical不起实际的&lt;/li&gt;
&lt;li&gt;explained in greater detail更详细地解释&lt;/li&gt;
&lt;li&gt;draw from从……提取&lt;/li&gt;
&lt;li&gt;we &lt;strong&gt;enumerate&lt;/strong&gt; guidelines枚举&lt;/li&gt;
&lt;li&gt;Rather than&lt;/li&gt;
&lt;li&gt;computationally cumbersome计算繁琐&lt;/li&gt;
&lt;li&gt;used extensively&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;excellent and comprehensive&lt;/strong&gt; review articles&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>2018-01-20-English-note</title>
      <link>/post/2018-01-20-english-note/</link>
      <pubDate>Sat, 20 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-01-20-english-note/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;seeding progeny&lt;/strong&gt; were &lt;strong&gt;raised in&lt;/strong&gt; containers&lt;/li&gt;
&lt;li&gt;stem cutting&lt;/li&gt;
&lt;li&gt;across a range of sites and conditions&lt;/li&gt;
&lt;li&gt;single-tree non-contiguous plots&lt;/li&gt;
&lt;li&gt;pilodyn penetration (PIL)用来测定木材密度&lt;/li&gt;
&lt;li&gt;are defined as above&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\otimes\)&lt;/span&gt; denotes the Kronecker product operation&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\oplus\)&lt;/span&gt; denotes the direct sum operation&lt;/li&gt;
&lt;li&gt;analysis of DBH&lt;/li&gt;
&lt;li&gt;assess the statistical significance评估统计显著性&lt;/li&gt;
&lt;li&gt;in previous single-site analyses预分析&lt;/li&gt;
&lt;li&gt;make use of利用&lt;/li&gt;
&lt;li&gt;take advantage of利用&lt;/li&gt;
&lt;li&gt;manipulate this problem处理问题&lt;/li&gt;
&lt;li&gt;to deal with this issue&lt;/li&gt;
&lt;li&gt;there is obvious evidence有明显的证据&lt;/li&gt;
&lt;li&gt;A considerable body of research on大量&lt;/li&gt;
&lt;li&gt;Theoretical work理论工作&lt;/li&gt;
&lt;li&gt;quantitative variation&lt;/li&gt;
&lt;li&gt;plays a &lt;strong&gt;pronounced&lt;/strong&gt; role in显著的&lt;/li&gt;
&lt;li&gt;empirical investigations实证研究，对应的是理论研究&lt;/li&gt;
&lt;li&gt;One of the reasons for this may be due to the fact that&lt;/li&gt;
&lt;li&gt;methodological limitation方法局限&lt;/li&gt;
&lt;li&gt;owing to由于&lt;/li&gt;
&lt;li&gt;a portion of一份&lt;/li&gt;
&lt;li&gt;a fraction of一部分&lt;/li&gt;
&lt;li&gt;be relaxed somewhat缓解&lt;/li&gt;
&lt;li&gt;in the case&lt;/li&gt;
&lt;li&gt;the question has not been explored这个问题尚未探索/探讨 staggered摇摇晃晃地 veiled &lt;strong&gt;chameleons&lt;/strong&gt;变色龙 masterpiece杰作 stunning极好的 sexually harass性骚扰 synopsis大纲 syllabus大纲 rejoice高兴 apostrophe撇&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>2018-01-20-ericjang</title>
      <link>/post/2018-01-20-ericjang/</link>
      <pubDate>Sat, 20 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-01-20-ericjang/</guid>
      <description>&lt;p&gt;Eric Jang&lt;/p&gt;
&lt;p&gt;Technology, A.I., Careers&lt;/p&gt;
&lt;p&gt;Wednesday, January 17, 2018 &lt;a href=&#34;http://blog.evjang.com/2018/01/nf1.html&#34;&gt;&lt;/a&gt; Normalizing Flows Tutorial, Part 1: Distributions and Determinants# E If you are a machine learning practitioner working on generative modeling, Bayesian deep learning, or deep reinforcement learning, normalizing flows are a handy technique to have in your algorithmic toolkit. Normalizing flows transform simple densities (like Gaussians) into rich complex distributions that can be used for generative models, RL, and variational inference. TensorFlow has a nice set of functions that make it easy to build flows and train them to suit real-world data.&lt;/p&gt;
&lt;p&gt;This tutorial comes in two parts: Part 1: Distributions and Determinants. In this post, I explain how invertible transformations of densities can be used to implement more complex densities, and how these transformations can be chained together to form a “normalizing flow”. Part 2: Modern Normalizing Flows: In a follow-up post, I survey recent techniques developed by researchers to learn normalizing flows, and explain how a slew of modern generative modeling techniques – autoregressive models, MAF, IAF, NICE, Real-NVP, Parallel-Wavenet – are all related to each other. This series is written for an audience with a rudimentary understanding of linear algebra, probability, neural networks, and TensorFlow. Knowledge of recent advances in Deep Learning, generative models will be helpful in understanding the motivations and context underlying these techniques, but they are not necessary.&lt;/p&gt;
&lt;p&gt;Background&lt;/p&gt;
&lt;p&gt;Statistical Machine Learning algorithms try to learn the structure of data by fitting a parametric distribution &lt;span class=&#34;math inline&#34;&gt;\(p(x; \theta)\)&lt;/span&gt; to it. Given a dataset, if we can represent it with a distribution, we can: Generate new data “for free” by sampling from the learned distribution in silico; no need to run the true generative process for the data. This is a useful tool if the data is expensive to generate, i.e. a real-world experiment that takes a long time to run [1]. Sampling is also used to construct estimators of high-dimensional integrals over spaces. Evaluate the likelihood of data observed at test time (this can be used for rejection sampling or to score how good our model is). Find the conditional relationship between variables. For example, learning the distribution &lt;span class=&#34;math inline&#34;&gt;\(p(x_2 | x_1)\)&lt;/span&gt; allows us to build discriminative classification or regression models. Score our algorithm by using complexity measures like entropy, mutual information, and moments of the distribution. We’ve gotten pretty good at sampling (1), as evidenced by recent work on generative models for images and audio. These kinds of generative models are already being deployed in real commercial applications and Google products.&lt;/p&gt;
&lt;p&gt;However, the research community currently directs less attention towards unconditional &amp;amp; conditional likelihood estimation (2, 3) and model scoring (4). For instance, we don’t know how to compute the support of a GAN decoder (how much of the output space has been assigned nonzero probability by the model), we don’t know how to compute the density of an image with respect to a DRAW distribution or even a VAE, and we don’t know how to analytically compute various metrics (KL, earth-mover distance) on arbitrary distributions, even if we know their analytic densities.&lt;/p&gt;
&lt;p&gt;Generating likely samples isn’t enough: we also care about answering “how likely is the data?” [2], having flexible conditional densities (e.g. for sampling/evaluating divergences of multi-modal policies in RL), and being able to choose rich families of priors and posteriors in variational inference.&lt;/p&gt;
&lt;p&gt;Consider for a moment, your friendly neighborhood Normal Distribution. It’s the Chicken Soup of distributions: we can draw samples from it easily, we know its analytic density and KL divergence to other Normal distributions, the central limit theorem gives us confidence that we can apply it to pretty much any data, and we can even backprop through its samples via the reparameterization trick. The Normal Distribution’s ease-of-use makes it a very popular choice for many generative modeling and reinforcement learning algorithms.&lt;/p&gt;
&lt;p&gt;Unfortunately, the Normal distribution just doesn’t cut it in many real-world problems we care about. In Reinforcement Learning – especially continuous control tasks such as robotics – policies are often modeled as multivariate Gaussians with diagonal covariance matrices.&lt;/p&gt;
&lt;p&gt;By construction, uni-modal Gaussians cannot do well on tasks that require sampling from a multi-modal distribution. A classic example of where uni-modal policies fail is an agent trying to get to its house across a lake. It can get home by circumventing the lake clockwise (left) or counterclockwise (right), but a Gaussian policy is not able to represent two modes. Instead, it chooses actions from a Gaussian whose mean is a linear combination of the two modes, resulting in the agent going straight into the icy water. Sad!&lt;/p&gt;
&lt;p&gt;The above example illustrates how the Normal distribution can be overly simplistic. In addition to bad symmetry assumptions, Gaussians have most of their density concentrated at the edges in high dimensions and are not robust to rare events. Can we find a better distribution with the following properties?&lt;/p&gt;
&lt;p&gt;Complex enough to model rich, multi-modal data distributions like images and value functions in RL environments? … while retaining the easy comforts of a Normal distribution: sampling, density evaluation, and with re-parameterizable samples? The answer is yes! Here are a few ways to do it: Use a mixture model to represent a multi-modal policy, where a categorical represents the “option” and the mixture represents the sub-policy. This provides samples that are easy to sample and evaluate, but samples are not trivially re-parameterizable, which makes them hard to use for VAEs and posterior inference. However, using a Gumbel-Softmax / Concrete relaxation of the categorical “option” would provide a multi-modal, re-parameterizable distribution. Autoregressive factorizations of policy / value distributions. In particular, discrete distributions (e.g. Categorical) have the ability to model arbitrary discrete distributions. In RL, one can avoid this altogether by symmetry-breaking the value distribution via recurrent policies, noise, or distributional RL. This helps by collapsing the complex value distributions into simpler conditional distributions at each timestep. Learning with energy-based models, a.k.a undirected graphical models with potential functions that eschew an normalized probabilistic interpretation. Here’s a recent example of this applied to RL. Normalizing Flows: learn invertible, volume-tracking transformations of distributions that we can manipulate easily.&lt;/p&gt;
&lt;p&gt;Let’s explore the last approach - Normalizing Flows.&lt;/p&gt;
&lt;p&gt;Change of Variables, Change of Volume&lt;/p&gt;
&lt;p&gt;Let’s build up some intuition by examining linear transformations of 1D random variables. Let &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; be the distribution &lt;span class=&#34;math inline&#34;&gt;\(\text{Uniform}(0,1)\)&lt;/span&gt;. Let random variable &lt;span class=&#34;math inline&#34;&gt;\(Y = f(X) = 2X + 1\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is a simple affine (scale &amp;amp; shift) transformation of the underlying “source distribution” &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. What this means is that a sample &lt;span class=&#34;math inline&#34;&gt;\(x^i\)&lt;/span&gt; from &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; can be converted into a sample from &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; by simply applying the function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; to it.&lt;/p&gt;
&lt;p&gt;The green square represents the shaded probability mass on &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}\)&lt;/span&gt; for both &lt;span class=&#34;math inline&#34;&gt;\(p(x)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(p(y)\)&lt;/span&gt; - the height represents the density function at that value. Observe that because probability mass must integrate to 1 for any distribution, the act of scaling the domain by 2 everywhere means we must divide the probability density by 2 everywhere, so that the total area of the green square and blue rectangle are the same (=1).&lt;/p&gt;
&lt;p&gt;If we zoom in on a particular x and an infinitesimally nearby point &lt;span class=&#34;math inline&#34;&gt;\(x+dx\)&lt;/span&gt;, then applying f to them takes us to the pair &lt;span class=&#34;math inline&#34;&gt;\((y, y+dy)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;On the left, we have a locally increasing function (&lt;span class=&#34;math inline&#34;&gt;\(dy/dx &amp;gt; 0\)&lt;/span&gt;) and on the right, a locally decreasing function (&lt;span class=&#34;math inline&#34;&gt;\(dy/dx &amp;lt; 0\)&lt;/span&gt;). In order to preserve total probability, the change of &lt;span class=&#34;math inline&#34;&gt;\(p(x)\)&lt;/span&gt; along interval &lt;span class=&#34;math inline&#34;&gt;\(dx\)&lt;/span&gt; must be equivalent to the change of &lt;span class=&#34;math inline&#34;&gt;\(p(y)\)&lt;/span&gt; along interval &lt;span class=&#34;math inline&#34;&gt;\(dy\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p(x) dx = p(y) dy\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In order to conserve probability, we only care about the amount of change in y and not its direction (it doesn’t matter if &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; is increasing or decreasing at x, we assume the amount of change in y is the same regardless). Therefore, &lt;span class=&#34;math inline&#34;&gt;\(p(y) = p(x) | dx/dy |\)&lt;/span&gt;. Note that in log-space, this is equivalent to &lt;span class=&#34;math inline&#34;&gt;\(\log p(y) = \log p(x) + \log | dx/dy |\)&lt;/span&gt;. Computing log-densities is more well-scaled for numerical stability reasons.&lt;/p&gt;
&lt;p&gt;Now let’s consider the multivariate case, with 2 variables. Again, zooming into an infinitesimally small region of our domain, our initial “segment” of the base distribution is now a square with width dx.&lt;/p&gt;
&lt;p&gt;Note that a transformation that merely shifts a rectangular patch &lt;span class=&#34;math inline&#34;&gt;\((x1,x2, x3,x4)\)&lt;/span&gt; does not change the area. We are only interested in the rate of change per unit area of x, so the displacement &lt;span class=&#34;math inline&#34;&gt;\(dx\)&lt;/span&gt; can be thought of as a unit of measure, which is arbitrary. To make the following analysis simple and unit-less, let’s investigate a unit square on the origin, i.e. 4 points &lt;span class=&#34;math inline&#34;&gt;\((0,0), (1,0), (0,1), (1,1)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Multiplying this by the matrix &lt;span class=&#34;math inline&#34;&gt;\([[a, b];[c, d]]\)&lt;/span&gt; will take points on this square into a parallelogram, as shown on the figure to the right (below). &lt;span class=&#34;math inline&#34;&gt;\((0,0)\)&lt;/span&gt; is sent to &lt;span class=&#34;math inline&#34;&gt;\((0,0)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\((1,0)\)&lt;/span&gt; is sent to &lt;span class=&#34;math inline&#34;&gt;\((a,b)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\((1,0)\)&lt;/span&gt; sent to &lt;span class=&#34;math inline&#34;&gt;\((c,d)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\((1,1)\)&lt;/span&gt; sent to &lt;span class=&#34;math inline&#34;&gt;\((a+c,b+d)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Thus, a unit square in the domain of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; corresponds to a deformed parallelogram in the domain of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, so the per-unit rate of change in area is the area of the parallelogram, i.e. &lt;span class=&#34;math inline&#34;&gt;\(ad - bc\)&lt;/span&gt;. The area of a parallelogram, &lt;span class=&#34;math inline&#34;&gt;\(ad - bc\)&lt;/span&gt;, is nothing more than the absolute value of the determinant of the linear transformation!&lt;/p&gt;
&lt;p&gt;In 3 dimensions, the “change in area of parallelogram” becomes a “change in volume of parallelpiped”, and even higher dimensions, this becomes “change in volume of a n-parallelotope”. But the concept remains the same - determinants are nothing more than the amount (and direction) of volume distortion of a linear transformation, generalized to any number of dimensions.&lt;/p&gt;
&lt;p&gt;What if the transformation f is nonlinear? Instead of a single parallelogram that tracks the distortion of any point in space, you can picture many infinitesimally small parallelograms corresponding to the amount of volume distortion for each point in the domain. Mathematically, this locally-linear change in volume is &lt;span class=&#34;math inline&#34;&gt;\(|\text{det}(J(f^{-1}(x)))|\)&lt;/span&gt;, where J(f^-1(x)) is the Jacobian of the function inverse - a higher-dimensional generalization of the quantity dx/dy from before.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y = f(x)\]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[p(y) = p(f^{-1}(y)) \cdot |\text{det} J(f^{-1}(y))| = \log p(f^{-1}(y)) + \log |\text{det}(J(f^{-1}(y)))|\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;When I learned about determinants in middle &amp;amp; high school I was very confused at the seemingly arbitrary definition of determinants. We were only taught how to compute a determinant, instead of what a determinant meant: the local, linearized rate of volume change of a transformation.&lt;/p&gt;
&lt;p&gt;Transformed Distributions in TensorFlow&lt;/p&gt;
&lt;p&gt;TensorFlow has an elegant API for transforming distributions. A TransformedDistribution is specified by a base distribution object that we will transform, and a Bijector object that implements 1) a forward transformation &lt;span class=&#34;math inline&#34;&gt;\(y = f(x)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(f : \mathbb{R}^d → \mathbb{R}^d\)&lt;/span&gt; 2) its inverse transformation &lt;span class=&#34;math inline&#34;&gt;\(x = f^-1(y)\)&lt;/span&gt;, and 3) the inverse log determinant of the Jacobian &lt;span class=&#34;math inline&#34;&gt;\(\log |\text{det}J (f^-1(y))|\)&lt;/span&gt;. For the rest of this post, I will abbreviate this quantity as ILDJ.&lt;/p&gt;
&lt;p&gt;Under this abstraction, forward sampling is trivial:&lt;/p&gt;
&lt;p&gt;bijector.forward(base_dist.sample())&lt;/p&gt;
&lt;p&gt;To evaluate log-density of the transformed distribution:&lt;/p&gt;
&lt;p&gt;distribution.log_prob(bijector.inverse(x)) + bijector.inverse_log_det_jacobian(x)&lt;/p&gt;
&lt;p&gt;Furthermore, if bijector.forward is a differentiable function, then Y = bijector.forward(x) is a re-parameterizable distribution with respect to samples x = base_distribution.sample(). This means that normalizing flows can be used as a drop-in replacement for variational posteriors in a VAE (as an alternative to a Gaussian).&lt;/p&gt;
&lt;p&gt;Some commonly used TensorFlow distributions are actually implemented using these TransformedDistributions.&lt;/p&gt;
&lt;p&gt;Source Distribution Bijector.forward Transformed Distribution&lt;/p&gt;
&lt;p&gt;Normal exp(x) LogNormal Exp(rate=1) -log(x) Gumbel(0,1) Gumbel(0,1) Softmax(x) Gumbel-Softmax / Concrete&lt;/p&gt;
&lt;p&gt;Under standard convention, TransformedDistributions are named as &lt;span class=&#34;math inline&#34;&gt;\(\text{Bijector}^{-1}\text{BaseDistribution}\)&lt;/span&gt; so an ExpBijector applied to a Normal distribution becomes LogNormal. There are some exceptions to this naming scheme - the Gumbel-Softmax distribution is implemented as the RelaxedOneHotCategorical distribution, which applies a SoftmaxCentered bijector to a Gumbel distribution.&lt;/p&gt;
&lt;p&gt;Normalizing Flows and Learning Flexible Bijectors&lt;/p&gt;
&lt;p&gt;Why stop at 1 bijector? We can chain any number of bijectors together, much like we chain layers together in a neural network [3]. This is construct is known as a “normalizing flow”. Additionally, if a bijector has tunable parameters with respect to bijector.log_prob, then the bijector can actually be learned to transform our base distribution to suit arbitrary densities. Each bijector functions as a learnable “layer”, and you can use an optimizer to learn the parameters of the transformation to suit our data distribution we are trying to model. One algorithm to do this is maximum likelihood estimation, which modifies our model parameters so that our training data points have maximum log-probability under our transformed distribution. We compute and optimize over log probabilities rather than probabilities for numerical stability reasons.&lt;/p&gt;
&lt;p&gt;This slide from Shakir Mohamed and Danilo Rezende’s UAW talk (slides) that illustrates this concept:&lt;/p&gt;
&lt;p&gt;However, computing the determinant of an arbitrary &lt;span class=&#34;math inline&#34;&gt;\(N \times N\)&lt;/span&gt; Jacobian matrix has runtime complexity &lt;span class=&#34;math inline&#34;&gt;\(O(N^3)\)&lt;/span&gt;, which is very expensive to put in a neural network. There is also the trouble of inverting an arbitrary function approximator. Much of the current research on Normalizing Flows focuses on how to design expressive Bijectors that exploit GPU parallelism during forward and inverse computations, all while maintaining computationally efficient ILDJs.&lt;/p&gt;
&lt;p&gt;Code Example&lt;/p&gt;
&lt;p&gt;Let’s build a basic normalizing flow in TensorFlow in about 100 lines of code. This code example will make use of:&lt;/p&gt;
&lt;p&gt;TF Distributions - general API for manipulating distributions in TF. For this tutorial you’ll need TensorFlow r1.5 or later. TF Bijector - general API for creating operators on distributions Numpy, Matplotlib.&lt;/p&gt;
&lt;p&gt;We are trying to model the distribution &lt;span class=&#34;math inline&#34;&gt;\(p(x_1, x_2) = \mathcal{N}(x1|\mu=1/4x_2^2, \sigma=1) \cdot N(x_2|\mu=0, \sigma=4)\)&lt;/span&gt;. We can generate samples from the target distribution using the following code snippet (we generate them in TensorFlow to avoid having to copy samples from the CPU to the GPU on each minibatch):&lt;/p&gt;
&lt;p&gt;For our base distribution, we’ll use an Isotropic Gaussian.&lt;/p&gt;
&lt;p&gt;Next, we construct the bijector and create a TransformedDistribution from it. Let’s build a flow that resembles a standard fully-connected network, i.e. alternating matrix multiplication with nonlinearities.&lt;/p&gt;
&lt;p&gt;The Jacobian of an affine function is trivial to compute, but worst case determinants are &lt;span class=&#34;math inline&#34;&gt;\(O(n^3)\)&lt;/span&gt;, which is unacceptably slow to compute. Instead, TensorFlow provides a structured affine transformation whose determinant can be computed more efficiently. This Affine transform is parameterized as a lower triangular matrix &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; plus a low rank update:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[M + V \cdot D \cdot V^T\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To compute &lt;span class=&#34;math inline&#34;&gt;\(\text{det}(M + V \cdot D \cdot V^T)\)&lt;/span&gt; cheaply, we use the matrix determinant lemma.&lt;/p&gt;
&lt;p&gt;Next, we need an invertible nonlinearity in order to express non-linear functions (otherwise the chain of affine bijectors remains affine). Sigmoid / tanh may seem like good choices, but they are incredibly unstable to invert - small changes in the output near -1 or 1 correspond to massive changes in input. In my experiments I could not chain 2 saturating nonlinearities together without gradients exploding. Meanwhile, ReLU is stable, but not invertible for &lt;span class=&#34;math inline&#34;&gt;\(x &amp;lt; 0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;I chose to implement PReLU (parameterized ReLU), which is the same as Leaky ReLU but with a learnable slope in the negative regime. The simplicity of PReLU and its straightforward Jacobian makes for a nice exercise in implementing your own custom Bijectors: notice that the ILDJ is 0 when &lt;span class=&#34;math inline&#34;&gt;\(x &amp;gt; 0\)&lt;/span&gt; (no volume change) and &lt;span class=&#34;math inline&#34;&gt;\(1/\alpha\)&lt;/span&gt; otherwise (compensating for the contraction in volume from multiplying x by &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;PReLU is an element-wise transformation, so the Jacobian is diagonal. The determinant of a diagonal matrix is just the product of the diagonal entries, so we compute the ILDJ by simply summing the diagonal entries of the log-Jacobian [4]. We build the “MLP Bijector” by using tfb.Chain(), then apply it to our base distribution to create the transformed distribution:&lt;/p&gt;
&lt;p&gt;Finally, we’ll train the model using Maximum Likelihood estimation: maximize the expected log probability of samples from the real data distribution, under our choice of model.&lt;/p&gt;
&lt;p&gt;We can visualize the (slow) deformation of space by coloring samples from base distribution according to their starting quadrant,&lt;/p&gt;
&lt;p&gt;And that’s it! TensorFlow distributions makes normalizing flows to implement, and automatically accumulate all the Jacobians determinants in a way that is clean and highly readable. Full source code for this post can be found on Github.&lt;/p&gt;
&lt;p&gt;You might notice that the deformation is rather slow, and it takes a lot of layers to learn a rather simple transformation [5]. In the next post, I will cover more modern techniques for learning normalizing flows.&lt;/p&gt;
&lt;p&gt;Acknowledgements&lt;/p&gt;
&lt;p&gt;I am very grateful to Dustin Tran for clarifying my understanding of normalizing flows, Luke Metz, Katherine Lee, and Samy Bengio for proofreading this post, and to Ben Poole, Rif A. Saurous, Ian Langmore for helping me to debug my code. You rock!&lt;/p&gt;
&lt;p&gt;Footnotes&lt;/p&gt;
&lt;p&gt;[1] The notion that we can augment our dataset with &lt;em&gt;new&lt;/em&gt; information from a finite set of data is a rather disturbing one, and it remains to be shown whether probabilistic machine learning can truly replace true generative processes (e.g. simulation of fluid dynamics), or whether at the end of the day it is only good for amortizing computation and any generalization we get on the training / test distribution is a lucky accident. [2] See A note on the evaluation of generative models for a thought-provoking discussion about how high log-likelihood is neither sufficient nor necessary to generate “plausible” images. Still, it’s better than nothing and in practice a useful diagnostic tool. [3]There’s a connection between Normalizing Flows and GANs via encoder-decoder GAN architectures that learn the inverse of the generator (ALI / BiGAN). Since there is a separate encoder trying to recover &lt;span class=&#34;math inline&#34;&gt;\(u = G^{-1}(X)\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(X = G(u)\)&lt;/span&gt;, the generator can be thought of as a flow for the simple uniform distribution. However, we don’t know how to compute the amount of volume expansion/contraction w.r.t. X, so we cannot recover density from GANs. However, it’s probably not entirely unreasonable to model the log-det-jacobian numerically or enforce some kind of linear-time Jacobian by construction. [4] The lemma “Determinant of diagonal matrices is the product of the diagonal entries” is quite intuitive from a geometric point of view: each dimension’s length distortion is independent of the other dimensions, so the total volume change is just the product of changes in each direction, as if we were computing the volume of a high-dimensional rectangular prism. [5] This MLP is rather limited in capacity because each affine transformation is only a 2x2 matrix, and the PReLU “warps” the underlying distribution very slowly (so several PreLUs are needed to bend the data into the right shape). For low dimensional distributions, this MLP is a very poor choice of a normalizing flow, and is meant for educational purposes.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>2018-01-20-purrr</title>
      <link>/post/2018-01-20-purrr/</link>
      <pubDate>Sat, 20 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-01-20-purrr/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Wu1996-克隆重复试验检测上位方差</title>
      <link>/post/2018-01-20-wu1996/</link>
      <pubDate>Sat, 20 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-01-20-wu1996/</guid>
      <description>&lt;div class=&#34;section level1&#34;&gt;
&lt;h1&gt;摘要&lt;/h1&gt;
&lt;p&gt;定量遗传模型是利用已知的克隆复制家族结构&lt;strong&gt;将遗传变异分解为其加性，显性和上位性成分&lt;/strong&gt;的一种定量遗传模型。利用后代测试，这个模型是基于这样一个理论：来自实验设计的线性模型的方差分量可以用亲属之间的期望协方差来表示。然而，&lt;strong&gt;如果一对数量性状位点（QTL）之间的相互作用解释了总体上位性的大部分，则将严重高估加性和显性差异，但低估了上位性差异。&lt;/strong&gt;在本文中，开发了一个新的模型来处理这个问题，将父母和子女的材料结合到同一个测试中。在上述条件下，新模型可以为加性x加性方差提供准确的估计。此外，其估计优势和总的上位变异的准确性远远大于以前的模型的准确性。但是，&lt;strong&gt;如果有明显证据显示高阶相互作用，尤其是&amp;gt;4个QTL对总上位性的主要贡献，以前的模型更适合将数量性状的遗传方差划分&lt;/strong&gt;。对杨树阶乘交配设计的一个例子的再分析显示，当使用两个不同的假设（低与高阶的上位相互作用）时，估计新模型和先前模型之间的方差分量存在很大差异。这个新模型将成为估计物种数量遗传模式的一种替代方法，特别是对长寿的，主要是异交的林木，可以进行克隆复制。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level1&#34;&gt;
&lt;h1&gt;介绍&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;上位性&lt;/strong&gt;，或者不同但功能相关基因座上的等位基因之间的遗传相互作用，是基因和特征之间非线性关系的结果（Falconer 1989）。生物化学，生理学和生物遗传学研究的大量研究强烈地表明，数量性状中无处不在的位点间互作（Hayman和Mather，1955; Hayman，1958; Wright，1980; Mather and Jinks，1982）。 Sewall Wright对进化论的最大贡献之一是显示&lt;strong&gt;上位性在表型进化和物种形成中如何起着核心作用&lt;/strong&gt;（Wright 1932,1980; Templeton 1979,1980; Carson and Tempteton 1984; Provine 1986; Wade 1992）。理论工作表明，&lt;strong&gt;野生种群的数量变异可能带有潜在的强上位性成分&lt;/strong&gt;（Crow和Kimura，1970），因为&lt;strong&gt;这些成分的小表型效应往往隐藏着更大的分子效应&lt;/strong&gt;。因此，&lt;strong&gt;当基因型频率受选择或种群瓶颈严重干扰时，神秘的上位变异可能变得重要&lt;/strong&gt;（Carson和Templeton 1984; Bryant等1986; Goodnight 1987,1988; Tachida和Cockerham 1989; Bryant和Meffert 1992; Wade 1992年）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;上位可能也是杂种优势的重要原因。&lt;/strong&gt; Jinks和Jones（1958）通过将不同的上位性成分整合到遗传模型中，观察到杂种优势的存在与否以及一年生植物中是否存在非等位相互作用。在Minvielle（1987）的双轨迹双列模型中，&lt;strong&gt;杂种优势可能来自乘性上位相互作用而不是显性效应&lt;/strong&gt;。 （Wright 1922; Dempster 1942; Ricky 1942; Williams 1959; Griffing 1990; Schnell and Cockerham 1992）研究了繁殖积累对复杂性状杂种优势的贡献。&lt;/p&gt;
&lt;p&gt;尽管上位性在进化和育种中起着显着的作用，但是对这种现象的实证研究惊人地少。造成这种现象的原因之一可能是由于&lt;strong&gt;分离世代以检测上位性不能在许多物种中获得&lt;/strong&gt;（Mather and Jinks 1982）。即使在目前的基于分子标记的图谱中，也存在处理这个问题的方法限制（Tanksley 1993）。 Fisher（1981）首先介绍了遗传方差分解为其加性，显性和上位性组分。 Cockerham（1954）将上位分量进一步细分为附加x加性，加性x显性和显性x显性的相互作用。所有这些&lt;strong&gt;相互作用可以根据世代均值来估计&lt;/strong&gt;（Hayman和Mather，1955）。福斯特（Foster）和肖（Shaw，1988）利用克隆从总遗传方差中分离上位性，通过表达亲缘关系期望协方差的实验设计线性模型的方差分量（Cockerham，1963）。由于其强大的性质，福斯特 - 肖的方法已被应用于许多森林树种（例如，Foster和Shaw 1988; Foster 1990; Mullin等1992; Mullin和Part 1994; R6nnberg-Wiistljung等1994）。 不幸的是，&lt;strong&gt;Foster-Shaw的方法不能提供遗传方差分量的准确估计&lt;/strong&gt;。一方面，&lt;strong&gt;半同胞或者同胞之间由协方差估计的加性和显性方差受到一部分上位性，尤其是低阶相互作用的污染&lt;/strong&gt;（Cockerham，1963）。另一方面，&lt;strong&gt;对上位变异的估计仅仅是家族变异所包含的上位性的一部分，而不是总的上位变异&lt;/strong&gt;。尽管假定多个位点之间有强的高度相互作用（至少_&amp;gt; 3）（Mullin和Park，1992），有偏倚的估计值可能会有所缓解，但在存在双位点相互作用的情况下，这将是误导。&lt;strong&gt;当低阶相互作用形成上位性的一部分时，估计的加性和显性方差将被严重污染，并且估计的上位性方差仅为总上位性的四分之一&lt;/strong&gt;。到目前为止，尚未探索基于不同交互类型如何准确排除上位性的问题。 在本文中，我试图通过在交配设计中结合父母和子代信息，在假设二基因上位 的情况下检测更精确的上位性估计。 从精确性分析和杨树实例可以看出，新方法可以作为将数量性状的遗传方差划分为可克隆物种的因果成分的替代方法。&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>2018-01-14-English-note</title>
      <link>/post/2018-01-14-english-note/</link>
      <pubDate>Sun, 14 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-01-14-english-note/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;data retrieval数据检索&lt;/li&gt;
&lt;li&gt;deemed to be视为&lt;/li&gt;
&lt;li&gt;irretrievable loss不可挽回的损失&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;patiently waiting&lt;/strong&gt; for &lt;strong&gt;optimum maturity&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Leftover lunch剩饭&lt;/li&gt;
&lt;li&gt;interactive visualizations&lt;/li&gt;
&lt;li&gt;Hello good &lt;strong&gt;people of Great Britain&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;When Rex Tillerson visited New Zealand last year, he &lt;strong&gt;stated&lt;/strong&gt; he had never seen so many middle fingers &lt;strong&gt;aimed at&lt;/strong&gt; his motorcade.&lt;/li&gt;
&lt;li&gt;Sometimes looks are &lt;strong&gt;deceiving&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;instant translate&lt;/li&gt;
&lt;li&gt;Impediments&lt;/li&gt;
&lt;li&gt;Nearly half of the &lt;strong&gt;residents&lt;/strong&gt; of low-income countries live more than one hour’s travel from a city, where vital resources such as education and health care are concentrated&lt;/li&gt;
&lt;li&gt;airport runway机场跑道&lt;/li&gt;
&lt;li&gt;Racial resentment种族仇恨&lt;/li&gt;
&lt;li&gt;Justice and dignity尊严, the endless shortage&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Excited to be joining&lt;/strong&gt; the … Biostatistics Department in February!&lt;/li&gt;
&lt;li&gt;hands-on&lt;/li&gt;
&lt;li&gt;ON MY DAY OFF休息日&lt;/li&gt;
&lt;li&gt;wearable sensor&lt;/li&gt;
&lt;li&gt;grant-in-aid资助款&lt;/li&gt;
&lt;li&gt;the &lt;strong&gt;aegis&lt;/strong&gt; of保护&lt;/li&gt;
&lt;li&gt;capitalize on利用&lt;/li&gt;
&lt;li&gt;tantamount to相当于&lt;/li&gt;
&lt;li&gt;in lieu of替代&lt;/li&gt;
&lt;li&gt;cheat sheet速查表&lt;/li&gt;
&lt;li&gt;Why might you do otherwise? 你为什么要这样做呢？&lt;/li&gt;
&lt;li&gt;sheepishly adv. 羞怯地；愚蠢地&lt;/li&gt;
&lt;li&gt;religiously adv. 虔诚地，笃信地&lt;/li&gt;
&lt;li&gt;highly internally consistent内部高度一致&lt;/li&gt;
&lt;li&gt;US authorities美国当局&lt;/li&gt;
&lt;li&gt;poring凝视&lt;/li&gt;
&lt;li&gt;transaction交易&lt;/li&gt;
&lt;li&gt;cash withdrawal现金提取&lt;/li&gt;
&lt;li&gt;inauguration就职&lt;/li&gt;
&lt;li&gt;There is nothing we can’t overcome.没有什么是我们解决不了的&lt;/li&gt;
&lt;li&gt;Donald J. Trump&lt;/li&gt;
&lt;li&gt;Let me be very clear here&lt;/li&gt;
&lt;li&gt;Today, we &lt;strong&gt;witnessed&lt;/strong&gt; an &lt;strong&gt;incredible&lt;/strong&gt; moment in history&lt;/li&gt;
&lt;li&gt;furiously猛烈地&lt;/li&gt;
&lt;li&gt;internship实习生&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>invoke|调用，引用</title>
      <link>/post/2018-01-14-invoke/</link>
      <pubDate>Sun, 14 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-01-14-invoke/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;&lt;p&gt;You create a command line object specifying the options (e.g. the input filename and the output filename), then &lt;strong&gt;invoke&lt;/strong&gt; this command line via a Python operating system call (e.g. using the subprocess module). (Chapman et al., 2000)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;By piping the result to wc we &lt;strong&gt;invoke&lt;/strong&gt; the word count program which indicates how many lines (i.e., rows) have the &amp;gt; symbol (this should be 101 for this example). (Pevsner, 2009)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Because of the high copy number of retrotransposons in the genomes of extant pine species, it is tempting to &lt;strong&gt;invoke&lt;/strong&gt; the mechanism of reverse transcription in the generation of complex pine gene families. (Kinlaw, Neale, 1997)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Proofs given in the statistical literature of the latter part either assume that = 0, which is a relatively easy case, or &lt;strong&gt;invoke&lt;/strong&gt; the same result by Laha (1956) that was mentioned earlier. (Khuri, 2009)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
